<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[投影，QR分解]]></title>
    <url>%2F2019%2F11%2F17%2F%E6%8A%95%E5%BD%B1%2F</url>
    <content type="text"><![CDATA[\(A\)是\(m\times n\)的矩阵，\(\mathbf{b}\in \mathbb{R}^m\)，求\(\mathbf{b}\)在\(C(A)\)上的投影\(\mathbf{p}\)。 \(\mathbf{p}\in C(A)\implies \exist\hat{\mathbf{x}}\in \mathbb{R}^n,A\hat{\mathbf{x}}=\mathbf{p}\)。 \(e=b-p\)垂直于\(C(A)\)，即\(e\in N(A^T)\)，即 \[ A^T(b-A\hat{x})=\mathbf{0} \] 即\(\hat{x}\)是： \[ A^TAx=A^Tb \] 的解。 这个方程总有解，因为\(C(A^T)=C(A^TA)\)。 如果\(A\)列满秩，那么有唯一解。 QR分解： 矩阵\(A\)的各列线性无关，通过Gram–Schmidt正交化过程得到正交归一的向量，作为矩阵\(Q\)的列： \[ Q=[e_1,\cdots,e_n] \] \[ a_k=\sum_{i=1}^k(a_i^Te_i)e_i \] 通过以上可得\(A=QR\)，其中\(R\)是上三角阵。]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[apache commons math的一些记录]]></title>
    <url>%2F2019%2F11%2F10%2FResizableDoubleArray%2F</url>
    <content type="text"><![CDATA[ResizableDoubleArray内部是一个数组，并且通过startIndex和numElements维护一个窗口。在addElementRolling时，除了numElements+1，startIndex也+1。默认情况下数组扩展时长度变为2倍，如果数组长度/元素个数大于2.5，则收缩数组。 DescriptiveStatistics内部使用ResizableDoubleArray，支持滑动窗口。计算统计量时，都是针对窗口当场计算，如计算方差： 123public double getVariance() &#123; return apply(varianceImpl);&#125; 1234public double apply(UnivariateStatistic stat) &#123; // eDA为内部的ResizableDoubleArray return eDA.compute(stat);&#125; 每种统计量实现为一个类，实现如下方法，在ResizableDoubleArray的compute方法中调用： 123double evaluate(double[] array, int startIndex, int numElements);]]></content>
  </entry>
  <entry>
    <title><![CDATA[naive bayes]]></title>
    <url>%2F2019%2F10%2F30%2Fnaive%20bayes%2F</url>
    <content type="text"><![CDATA[生成模型：\(p(X|Y)\)描述了如何根据目标属性\(Y\)来生成随机实例\(X\)。 目的是计算\(p(Y|X_1\dots X_n)\)来对\(Y\)分类。 这里并不存在条件独立，即\(p(Y|X_1\dots X_n)\ne\prod p(Y|X_i)\)，那样不是证据越多反而概率越小了吗？ 所以先通过Bayes定理： \[ p(Y|X_1\dots X_n)=\frac{p(Y)p(X_1\dots X_n|Y)}{p(X_1\dots X_n)} \] 假设\(Y,X_i\)都是二值的，那么\(p(X_1\dots X_n|Y)\)总共有\(2(2^n-1)\)个参数，要估计这些参数需要的数据太多。因此要引入其他假设。 这里就用条件独立： \[ p(X_i|X_j,Y)=p(X_i|Y) \] 那么 \[ p(X_1\dots X_n|Y)=\prod_{i=1}^np(X_i|Y) \] 参数数量降为\(2n\). 如果要求最可能的\(Y\)，由于分母和\(Y\)无关，因此： \[ Y\leftarrow \arg\max_{y_k}p(Y=y_k)\prod_ip(X_i|Y=y_k) \] 参数的极大似然估计（通过拉格朗日乘子推导？）： \[ \hat{p}(X_i=x_{ij})|Y=y_k)=\frac{\#D\{X_i=x_{ij} \bigwedge Y=y_k\}}{\#D\{Y=y_k\}} \] \(x_{ij}\)表示\(X_i\)取第\(j\)个值，\(\#D\{x\}\)表示数据集中满足条件\(x\)的数据数量。 如果没有满足条件的数据，相应的概率会估计为0。可以通过虚拟的数据来平滑： \[ \hat{p}(X_i=x_{ij})|Y=y_k)=\frac{\#D\{X_i=x_{ij} \bigwedge Y=y_k\}+l}{\#D\{Y=y_k\}+lJ} \] 其中\(J\)是\(X_i\)能取的值的个数。如果\(l=1\)，称为Laplace平滑。 \[ \hat{p}(Y=y_k)=\frac{\#D\{Y=y_k\}}{|D|} \] 平滑的估计： \[ \hat{p}(Y=y_k)=\frac{\#D\{Y=y_k\}+l}{|D|+lK} \] 其中\(K\)为\(Y\)能取的值的个数。 Naive Bayes for Continuous Inputs 如果\(X_i\)是连续变量，可将\(p(X_i|Y)\)建模为高斯分布。 NB是线性分类器（线性分类器是针对二分类，正类/负类由一个超平面分隔） Logistic Regression 正则化：\(-\lambda\|W\|^2\)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[AdaBoost]]></title>
    <url>%2F2019%2F10%2F30%2Fadaboost%2F</url>
    <content type="text"><![CDATA[算法推导 目标函数是基函数的线性组合： \[ f(x)=\sum_{m=1}^M\alpha_m b(x;\beta_m) \] loss函数最小化： \[ \min_{\{\alpha_m,\beta_m\}_i^M}\sum_{i=1}^NL\left(y_i,\sum_{m=1}^M\alpha_m b(x_i;\beta_m)\right) \] 但是这个很难求。Forward Stagewise方法每次只对一组\((\alpha_m,\beta_m)\)做最小化，求完之后这组参数就固定了。 adaboost中\(y\in\{-1,1\}\)，loss函数为指数函数： \[ L(y,f(x))=e^{-yf(x)} \] 基函数（分类器）为\(G_m(x)\in\{-1,1\}\)。 由前\(m\)个基函数得到的分类器为： \[ f_m(x)=f_{m-1}(x)+\alpha_mG_m(x) \] 这里\(\alpha_m&gt;0\)。 由Forward Stagewise方法， \[ \begin{align} (\alpha_m,G_m)&amp;=\arg\min_{\alpha,G} L(y,f_m(x))\\ &amp;=\arg\min_{\alpha,G}\sum_{i=1}^N\exp[-y_i(f_{m-1}(x_i)+\alpha G(x_i))]\\ &amp;=\arg\min_{\alpha,G}\sum_{i=1}^Nw_m^{(i)}\exp(-y_i\alpha G(x_i))\quad\left(w_m^{(i)}\triangleq \exp(-y_if_{m-1}(x_i))\right) \end{align} \] \(w_m^{(i)}\)和\(\alpha,G(x)\)无关，可以看成是数据点的权重。初始时所有数据点的权重相等，都为\(\frac{1}{N}\)。权重可以递推更新： \[ w_{m+1}^{(i)}=\frac{w_m^{(i)}}{Z_m}\exp(-y_i\alpha_mG_m(x_i)) \] 其中\(Z_m=\sum_{i=1}^Nw_m^{(i)}\exp(-y_i\alpha_mG_m(x_i))=L(y,f_m(x))\)是规范化因子，保证\(w_m\)成为概率分布。 上式又可写为： \[ w_{m+1}^{(i)}=\begin{cases} \frac{w_m^{(i)}}{Z_m}e^{-\alpha_m}, &amp; G_m(x_i)=y_i \\ \frac{w_m^{(i)}}{Z_m}e^{\alpha_m}, &amp; G_m(x_i)\neq y_i \end{cases} \] 可见被\(G_m(x)\)误分类的样本在下一轮中权重扩大，使得分类器能重点关注这些样本。 上面的loss函数可化为（为简化，符号中去掉了\(m\)）： \[ e^{-\alpha}\sum_{y_i=G(x_i)}w^{(i)}+e^{\alpha}\sum_{y_i\neq G(x_i)}w^{(i)} \] 加权误分类率\(e_m=\sum_{y_i\neq G(x_i)}w^{(i)}\)，上式变为： \[ (e^\alpha-e^{-\alpha})e_m+e^{-\alpha} \] 给定\(\alpha\)，上式第二项为常数，第一项前面的系数非负，因此： \[ \arg\min_{G(x)} L = \arg\min_{G(x)}e_m \] 即\(G_m(x)\)要在加权数据上误分类率最低。 对\(\alpha\)求最小化，解得： \[ \alpha = \frac{1}{2}\ln\frac{1-e_m}{e_m} \] 对于弱分类器\(G_m(x)\)，要求其要好于随机猜，即\(e_m=\frac{1}{2}-\gamma_m&lt;\frac{1}{2}\)。 将\(\alpha\)带入\(L(y,f_m(x))\)，得： \[ L(y,f_m(x))=Z_m=2\sqrt{e_m(1-e_m)} \] 训练误差分析 训练集分类错误率： \[ \frac{1}{N}\sum_{i=1}^NI(\text{sign}(f(x_i))\neq y_i) \] 其中\(I(\cdot)\)为indicator function。 由于 \[ I\left(\text{sign}(f(x_i))\neq y_i\right)=I(y_if(x_i)\le 0)\le \exp(-y_if(x_i)) \] 因此推导\(\frac{1}{N}\sum_{i=1}^N\exp(-y_if(x_i))\)的界。 \[ \begin{align} \frac{1}{N}\sum_i\exp(-y_if(x_i))&amp;=\frac{1}{N}\sum_i\exp\left(-\sum_{m=1}^My_i\alpha_mG_m(x_i)\right)\\ &amp;=\sum_iw_1^{(i)}\prod_{m=1}^M\exp(-y_i\alpha_mG_m(x_i))\quad(w_1^{(i)}=\frac{1}{N})\\ &amp;=\sum_iw_1^{(i)}\exp(-y_i\alpha_1G_1(x_i))\prod_{m=2}^M\exp(-y_i\alpha_mG_m(x_i))\\ &amp;=\sum_iZ_1w_2^{(i)}\prod_{m=2}^M\exp(-y_i\alpha_mG_m(x_i))\\ &amp;=Z_1\sum_iw_2^{(i)}\prod_{m=2}^M\exp(-y_i\alpha_mG_m(x_i))\\ &amp;=\cdots\\ &amp;=\prod_{m=1}^MZ_m\\ &amp;=\prod_{m=1}^M2\sqrt{e_m(1-e_m)}\\ &amp;=\prod_{m=1}^M\sqrt{1-4\gamma_m^2}\\ &amp;\le\exp\left(-2\sum_{i=1}^M\gamma_m^2\right) \end{align} \] 因此如果存在\(\gamma&gt;0\)，使得对所有\(m\)有\(\gamma_m\ge \gamma\)，则： \[ \frac{1}{N}\sum_{i=1}^NI(\text{sign}(f(x_i))\neq y_i)\le e^{-2M\gamma^2} \] 这表明训练误差以指数速度下降，最终降为0。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[线性代数中的一些结论]]></title>
    <url>%2F2019%2F10%2F29%2F%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E4%B8%AD%E7%9A%84%E4%B8%80%E4%BA%9B%E7%BB%93%E8%AE%BA%2F</url>
    <content type="text"><![CDATA[设\(\mathbf{v}_1,\cdots,\mathbf{v}_n\)是\(\mathbb{R}^n\)的一组基，\(A\)是\(n\times n\)的可逆矩阵，则\(A\mathbf{v}_1,\cdots,A\mathbf{v}_n\)也是\(\mathbb{R}^n\)的一组基。 \(N(A)=N(A^TA)\)：因为 \[ Ax=\mathbf{0}\implies A^TAx=\mathbf{0} \] \[ A^TAx=\mathbf{0}\implies x^TA^TAx=(Ax)^T(Ax)=\mathbf{0}\implies Ax=\mathbf{0} \] 那么\(r(A^T)=r(A)=r(A^TA)\)，由此可得\(C(A^T)=C(A^TA)\)。]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[线性方程组的解]]></title>
    <url>%2F2019%2F10%2F29%2F%E7%BA%BF%E6%80%A7%E6%96%B9%E7%A8%8B%E7%BB%84%E7%9A%84%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[设\(x^*\)是\(Ax=b\)的一个特解，那么\(Ax=b\)的解由\(x^*\)加上\(Ax=0\)的解构成。 设\(A\)是\(m\times n\)的矩阵，秩为\(r\)。 有4种情形： 1. \(m=n=r\) 此时\(A\)可逆，有唯一解。 2. \(r=n&lt;m\) \(A\)的列线性无关，如果\(b\)在列空间中，有唯一解，否则无解。 3. \(r=m&lt;n\) \(A\)转换为简化行阶梯形后可知有\(n-m\)个自由变量，因为有无穷多解。 4. \(r&lt;m,r&lt;n\) \(A\)经初等行变换\(E\)化为简化行阶梯形\(U_0\)，再通过交换列变为如下矩阵\(R\)： \[ R=EAP=\begin{pmatrix}I_r &amp; F\\0 &amp; 0\end{pmatrix} \] \[ \begin{align} Ax=b&amp;\iff EAx=Eb\\ &amp;\iff EAPP^{-1}x=d\quad(d\triangleq Eb)\\ &amp;\iff Ry=d \quad(y\triangleq P^{-1}x) \end{align} \] \[ d=\begin{pmatrix}d_1\\\vdots\\d_r\\d_{r+1}\\\vdots\\d_m\end{pmatrix} \] 显然只有当\(d_{r+1}=\cdots=d_m=0\)时才有解，且根据情形3，此时有无穷多解。]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[矩阵的秩]]></title>
    <url>%2F2019%2F10%2F29%2F%E7%9F%A9%E9%98%B5%E7%9A%84%E7%A7%A9%2F</url>
    <content type="text"><![CDATA[行秩=列秩 矩阵\(A\)经初等行变换\(E\)化为简化行阶梯形\(U_0\)，再通过交换列变为如下矩阵： \[ R=\begin{pmatrix}I_r &amp; F\\0 &amp; 0\end{pmatrix} \] 显然\(R\)的行秩、列秩都等于\(r\)。由于\(EA\)只是对\(A\)的行做线性组合，因此不改变行秩，即\(A\)的行秩等于\(r\)。 另外 \[ Ax=0\iff (EA)x=0 \] 因此初等行变换前后列之间的依赖关系不变，即列秩不变。 因此\(A\)的行秩、列秩都等于\(r\)。 性质 \[ r(AB)\le min(r(A),r(B)) \] 证明： \(r(AB)=dim\,C(AB)\)（秩等于列空间的维数）。 \(C(AB)\subset C(A)\implies dim\,C(AB)\le dim\,C(A)=r(A)\)，则\(r(AB)\le r(A)\)。 同理，\(r(B^TA^T)\le r(B^T)\implies r(AB)\le r(B)\) \[ r(A+B)\le r(A)+r(B) \] 因为\(C(A+B)\subset C(\begin{pmatrix}A&amp; B\end{pmatrix})\) 如果： \(A=B_1B_2\)，\(B_1\)是\(n\times r\)阶阵，\(B_2\)是\(r\times n\)阶阵，\(r(B_1)=r(B_2)=r\)， 那么： \(r(A)=r\)]]></content>
      <categories>
        <category>线性代数</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[CART]]></title>
    <url>%2F2019%2F10%2F15%2FCART%2F</url>
    <content type="text"><![CDATA[这里说的树都是二叉树。 结点\(t\)代表输入空间中的一块区域。\(p(t)\)为随机采样一个数据点，这个点在\(t\)中的概率。 对于\(K\)个类的概率（满足\(p_i\ge0,\sum_{i=1}^K p_i=1\)），定义impurity函数\(\phi(p_1,\cdots,p_K)\)，如entropy，Gini index。 定义结点\(t\)的impurity（measure）\(i(t)=\phi(p(1|t),\cdots,p(K|T))\) 划分\(\mathfrak{s}\)将\(t\)分为\(t_L,t_R\)两个子节点，定义\(p_L=p(t_L)/p(t)\)，类似定义\(p_R\)，显然\(p_L+p_R=1\)。定义impurity的减少为： \[ \Delta i(\mathfrak{s},t)=i(t)-p_Li(t_L)-p_Ri(t_R) \] 结点\(t\)的impurity：\(I(t)=i(t)p(t)\) \(T\)表示一棵树，\(\tilde{T}\)表示叶结点集合，\(T\)的impurity： \[ I(T)=\sum_{t\in\tilde{T}}I(t) \] 定义 \[ \Delta I(\mathfrak{s},t)=I(t)-T(t_L)-I(t_R) \] 可得： \[ \Delta I(\mathfrak{s},t)=\Delta i(\mathfrak{s},t)p(t) \] 因此使得两者最大化的划分是一样的。 \(C(i|j)\)表示将类\(j\)误分为类\(i\)的代价（非负），那么将节点\(t\)标记为类\(i\)的代价为： \[ \sum_j C(i|j)p(j|t) \] \(j^*(t)\)定义为使上式最小的类。\(r(t)\)定义为上式的最小值。 令\(R(t)=r(t)p(t)\)，定义树\(T\)的误分类代价为： \[ R(T)=\sum_{t\in\tilde{T}}R(t) \] 可以证明划分节点会降低代价： \[ R(t)\ge R(t_L)+R(t_R) \] 但是这样一直划分下去，每个结点只包含一个类。这样会过拟合。一种方式是限制叶结点的数目。 令\(\alpha\ge0\)，对于任意结点\(t\)（不一定是叶结点），定义： \[ R_\alpha(t)=R(t)+\alpha \] 对于树\(T\)，定义： \[ R_\alpha(T)=\sum_{t\in\tilde{T}}R(t)=R(T)+\alpha|\tilde{T}| \] 这样由于划分会增大\(\alpha|\tilde{T}|\)，代价不一定会减小。 定义\(T\)的pruned subtree \(T_1\)，记为\(T_1\preceq T\)：选取\(T\)的一个结点\(t\)，删除\(t\)的所有后代结点（保留\(t\)，因此所有叶结点仍然形成对输入空间的完整划分）。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[信息论-熵]]></title>
    <url>%2F2019%2F10%2F14%2F%E4%BF%A1%E6%81%AF%E8%AE%BA-%E7%86%B5%2F</url>
    <content type="text"><![CDATA[熵： \[ H(X)=-\sum_{x\in\mathcal{X}} p(x)\log p(x) \] 联合熵： \[ H(X,Y)=-\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x,y)\log p(x,y) \] 条件熵：给定条件变量\(X\)取值\(x\)，可以求\(H(Y|X=x)\)，然后对\(X\)求期望 \[ \begin{align} H(Y|X)&amp;=\sum_{x\in\mathcal{X}}p(x)H(Y|X=x)\\ &amp;=-\sum_{x\in\mathcal{X}}p(x)\sum_{y\in\mathcal{Y}}p(y|x)\log p(y|x)\\ &amp;=-\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x,y)\log p(y|x) \end{align} \] \(H(X,Y)=H(Y|X)+H(X)\) 证明： \[ \begin{align} H(X,Y)&amp;=-\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x,y)\log p(x,y)\\ &amp;=-\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x,y)\log \Big(p(y|x)p(x)\Big)\\ &amp;=-\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x,y)\Big(\log p(y|x)+\log p(x)\Big)\\ &amp;=H(Y|X)-\sum_{x\in\mathcal{X}}\log p(x)\sum_{y\in\mathcal{Y}}p(x,y)\\ &amp;=H(Y|X)-\sum_{x\in\mathcal{X}}p(x)\log p(x)\\ &amp;=H(Y|X)+H(X) \end{align} \] 可进一步推广为链式法则： \[ H(X_1,X_2,\dots,X_n)=\sum_{i=1}^nH(X_i|X_{i-1},\dots,X_1) \] 相对熵（也叫Kullback–Leibler距离 ）度量两个随机分布之间的距离：对于概率分布\(p(x),q(x)\) \[ D(p\|q)=\sum_{x\in\mathcal{X}}p(x)\frac{p(x)}{q(x)} \] 相对熵总是非负的，当且仅当\(p=q\)时为0（由Jensen不等式证明）。但是相对熵并不对称，也不满足三角不等式，所以不是metric。 互信息：是联合分布和乘积分布\(p(x)p(y)\)的相对熵： \[ \begin{align} I(X;Y)&amp;=\sum_{x\in\mathcal{X}}\sum_{y\in\mathcal{Y}}p(x,y)\log\frac{p(x,y)}{p(x)p(y)}\\ &amp;=D(p(x,y)\|p(x)p(y)) \end{align} \] \[ \begin{align} I(X;Y)&amp;=\sum_{x,y}p(x,y)\log\frac{p(x,y)}{p(x)p(y)}\\ &amp;=\sum_{x,y}p(x,y)\log\frac{p(x|y)}{p(x)}\\ &amp;=-\sum_{x,y}p(x,y)\log p(x)+\sum_{x,y}p(x,y)\log p(x|y)\\ &amp;=H(X)-H(X|Y) \end{align} \] 因此互信息是由\(Y\)的知识造成的\(X\)的不确定度的缩减。由于互信息是相对熵，因此非负，当且仅当\(p(x,y)=p(x)p(y)\)，即\(X,Y\)独立时为0。 由\(I(X;Y)=I(Y;X)=H(Y)-H(Y|X)\)，\(X,Y\)对彼此的提供的信息量是一样的，这也许就是mutual的含义。 由之前推导还可得： \[ I(X;Y)=H(X)+H(Y)-H(X,Y)\\ I(X;X)=H(X)-H(X|X)=H(X) \]]]></content>
  </entry>
  <entry>
    <title><![CDATA[决策树]]></title>
    <url>%2F2019%2F10%2F14%2F%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[选择划分属性 information gain 就是信息论中的互信息： \[ H(Y)-H(Y|a) \] 其中\(a\)为属性。 如果\(a\)取值较多，那么\(H(Y|a)\)可能会偏小。极端的例子为\(a\)为样例编号，那么\(H(Y|a)=0\)。 C4.5中使用的是gain ratio，也就是： \[ \frac{H(Y)-H(Y|a)}{H(a)} \] 其中\(H(a)\)为数据集关于属性\(a\)的熵。\(a\)取值越多，值可能越分散，因而\(H(a)\)可能越大。 C4.5先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。 Gini index \[ \text{Gini}(D)=1-\sum_{k=1}^{|\mathcal{Y}|}p_k^2 \] 其中\(p_k\)表示第\(k\)类的概率。 基尼指数反映了从数据集\(D\)中随机抽取两个样本，其类别标记不同的概率。因此越小数据集的纯度越高。 属性\(a\)的基尼指数： \[ \text{Gini}(D,a)=\sum_{v=1}^V\frac{|D^v|}{|D|}\text{Gini}(D^v) \] 其中\(D^v\)为属性\(a\)的取值为第\(v\)个值的数据。 那么选取的属性为： \[ a_* = \text{argmin}_{a\in A}\text{Gini}(D,a) \] 剪枝 剪枝的基本策略有“预剪枝”和“后剪枝”。 预剪枝：在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来泛化性能提升，则停止划分并将当前结点标记为叶节点。 后剪枝：先从训练集生成一颗完整的决策树，然后自底向上对非叶结点进行考察，若将该结点对应的子树替换为叶结点能提升泛化性能，则替换。 预剪枝使得决策树的很多分支没有展开，降低了过拟合的风险，但这是基于贪心的策略（后续划分可能导致性能提高），带来了欠拟合的风险。 连续属性 假设\(a\)在\(D\)中出现了\(n\)个不同的取值，将这些值从小到大排序，记为\(\{a^1,a2,\dots,a^n\}\)。基于划分点\(t\)可分为不大于和大于的集合。\(t\)的取值集合为： \[ T_a=\{\frac{a^i+a^{i+1}}{2}|1\le i\le n-1\} \] 若当前结点划分属性为连续属性，该属性还可以作为其后代结点的划分属性。 缺失值处理 两个问题： 如何在属性值缺失的情况下选择划分属性？ 给定划分属性，若某样本在该属性上的值缺失，如果将样本划分到子节点？ 用\(\tilde{D}\)表示\(D\)中在属性\(a\)上没有缺失值的样本子集。 对于问题1，可以在\(\tilde{D}\)中计算指标（比如信息增益），然后乘以\(\tilde{D}\)相对于\(D\)的权重（由于问题2，样本的权重会变，所以并不只是样本的比例），表示对指标的置信程度。 对于问题2，如果样本\(x\)在\(a\)上的值已知，则划入对应的子节点。如果取值缺失，则将\(x\)同时划入所有子节点，且样本权重按子节点的比例调整。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[平面最近点对问题]]></title>
    <url>%2F2019%2F10%2F13%2F%E5%B9%B3%E9%9D%A2%E6%9C%80%E8%BF%91%E7%82%B9%E5%AF%B9%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[分治法 竖直线\(l\)将点集平分为两部分，\(d_L,d_R\) 分别表示左右两边的点集的最小距离，令\(\delta = \min(d_L,d_R)\)。为了使一个点在左，一个在右的距离小于\(\delta\)，只需在\(l\)两边宽度为\(\delta\)的条带中找。如下图所示 closest-pair-1 同时两个点在竖直方向上距离不能超过\(\delta\)，因此只需在\(\delta\times 2\delta\)的框内找。考虑到左边的\(\delta\times \delta\)的框内两点间的距离不能超过\(\delta\)，因此最多只能有4个点。右边同理。如下图所示 closest-pair-2 因此在按y坐标排序的点集中，只需计算随后的7个点。这样计算由分别在左右的点构成的点对的时间为\(O(n)\)，整个算法的复杂度因此为\(O(n\log n)\)。 参考资料 算法导论 第三版]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[算法时间复杂度渐进符号]]></title>
    <url>%2F2019%2F10%2F13%2F%E7%AE%97%E6%B3%95%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6%E6%B8%90%E8%BF%9B%E7%AC%A6%E5%8F%B7%2F</url>
    <content type="text"><![CDATA[上界： \(T(n)=O(f(n))\)当且仅当存在正的常数\(c,n_0\)，使得： \[ T(n)\le c\cdot f(n)\quad \forall n&gt;n_0 \] 下界： \(T(n)=\Omega(f(n))\)当且仅当存在正的常数\(c,n_0\)，使得： \[ T(n)\ge c\cdot f(n)\quad \forall n&gt;n_0 \] 既是上界也是下界： \(T(n)=\Theta(f(n))\)当且仅当存在正的常数\(c_1,c_2,n_0\)，使得： \[ c_1\cdot f(n)\le T(n)\le c_2\cdot f(n) \quad\forall n&gt;n_0 \] \(T(n)=o(f(n))\)当且仅当对于任意常数\(c&gt;0\)，存在\(n_0\)，使得： \[ T(n)\le c\cdot f(n)\quad \forall n&gt;n_0 \]]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Karatsuba算法]]></title>
    <url>%2F2019%2F10%2F13%2FKaratsuba%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[Karatsuba算法是一种快速乘法算法。设\(x,y\)均为\(n\)为整数（这里假设\(n\)为偶数），求\(x\cdot y\)。直接乘复杂度为\(O(n^2)\)。 用分治法，其中 \[ x=a\cdot 10^{n/2}+b\\ y=c\cdot 10^{n/2}+d \] 那么 \[ \begin{align} x\cdot y &amp;= (a\cdot 10^{n/2}+b)(c\cdot 10^{n/2}+d)\\ &amp;=ac\cdot10^n+(ad+bc)\cdot10^{n/2}+bd \end{align} \] 这里是4次大小为\(n/2\)的乘法，并不能降低复杂度。关键在于 \[ ad+bc=(a+b)(c+d)-ac-bd \] 这样就只有3次乘法，降低了复杂度。]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[numpy]]></title>
    <url>%2F2019%2F10%2F03%2Fnumpy%2F</url>
    <content type="text"><![CDATA[ndarray的data在内存中是连续存储的，通过strides属性来将索引映射到存储位置。strides是一个长度为ndim的tuple，表示相应轴上前进一个单位对应内存位置上增加的字节数。如果用\((s_1,\dots,s_n)\)表示，索引用\(i_1,\dots,i_n\)表示，那么对应的内存字节位置为\(\sum_{k=1}^ni_ks_k\)。 有的操作只需要改变strides，引用的还是原来的data，这样的数组称为view。 np.full：用任意常数构成数组 np.logspace：和linspace类似，只不过是作为指数。 numpy.ones_like等函数： 以另一个数组为参数，使得生成的数组有同样的shape和dtype。 通过切片得到的数组是view，因此改变这个数组会改变原数组。如果想要独立的数组，用copy。 fancy indexing：可以用整形数组和列表来作为索引。 还可以用bool数组作为索引，选中为True的位置。 通过fancy indexing和bool数组方式得到的是独立的数组，而不是view。但是可以通过这两种方式来改变数组： 12345678910# fancy indexingIn [99]: A = np.arange(10)In [100]: indices = [2, 4, 6]In [101]: B = A[indices]In [102]: B[0] = -1 # this does not affect AIn [103]: AOut[103]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])In [104]: A[indices] = -1 # this alters AIn [105]: AOut[105]: array([ 0, 1, -1, 3, -1, 5, -1, 7, 8, 9]) 123456789# Boolean-valued indexingIn [106]: A = np.arange(10)In [107]: B = A[A &gt; 5]In [108]: B[0] = -1 # this does not affect AIn [109]: AOut[109]: array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])In [110]: A[A &gt; 5] = -1 # this alters AIn [111]: AOut[111]: array([ 0, 1, 2, 3, 4, 5, -1, -1, -1, -1]) reshape创建view。 ravel将数组展开成一维，创建view。 flatten同样得到一维数组，但是是原数组的copy。 np.newaxis填充一个长度为1的轴。 broadcast 两个数组如果维度不同，则维度较少的那个数组从左边开始填充长度为1的轴。之后两个数组相应轴比较，如果长度相等或者有一个长度为1，则能够应用broadcast。]]></content>
      <tags>
        <tag>numpy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[支持向量机]]></title>
    <url>%2F2019%2F09%2F21%2F%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[hard-margin 线性可分 假设数据点是线性可分的，即存在\((b,\mathbf{w})\)满足 \[ y_n(\mathbf{w}^T\mathbf{x}_n+b)&gt;0 \quad n=1,\dots,N \] \((b,\mathbf{w})\)和\((\frac{b}{\rho},\frac{\mathbf{w}}{\rho})\)表示同样的超平面。令 \[ \rho = \min_{n=1,\dots,N}y_n(\mathbf{w}^T\mathbf{x}_n+b) \] 则对于同样的超平面\((b/\rho,\mathbf{w}/\rho)\)，有 \[ \min_{n=1,\dots,N}y_n\left(\frac{\mathbf{w}^T}{\rho}\mathbf{x}_n+\frac{b}{\rho}\right)=\frac{1}{\rho}\min_{n=1,\dots,N}y_n(\mathbf{w}^T\mathbf{x}_n+b)=\frac{\rho}{\rho}=1 \] 因此可定义超平面\(h=(b,\mathbf{w})\)满足 \[ \min_{n=1,\dots,N}y_n(\mathbf{w}^T\mathbf{x}_n+b)=1 \] 点\(\mathbf{x}\)到\(h\)的距离为 \[ d(\mathbf{x},h)=\frac{|\mathbf{w}^T\mathbf{x}+b|}{\|\mathbf{w}\|} \] 由于\(y_n=\pm1\)且线性可分 \[ |\mathbf{w}^T\mathbf{x}+b|=|y_n(\mathbf{w}^T\mathbf{x}+b)|=y_n(\mathbf{w}^T\mathbf{x}+b) \] 那么 \[ d(\mathbf{x}_n,h)=\frac{y_n(\mathbf{w}^T\mathbf{x}_n+b)}{\|\mathbf{w}\|} \] 所有点到超平面距离的最小值称为margin： \[ \min_{n=1,\dots,N}d(\mathbf{x}_n,h)=\frac{1}{\|\mathbf{w}\|}\cdot\min_{n=1,\dots,N}y_n(\mathbf{w}^T\mathbf{x}_n+b)=\frac{1}{\|\mathbf{w}\|} \] 我们希望得到margin最大的超平面。让\(\frac{1}{\|\mathbf{w}\|}\)最大也就是让\(\mathbf{w}^T\mathbf{w}\)最小，因此问题归结于： \[ \begin{aligned} \min_{b,\mathbf{w}}&amp;\quad\frac{1}{2}\mathbf{w}^T\mathbf{w}\\ s.t.&amp;\quad\min_{n=1,\dots,N}y_n(\mathbf{w}^T\mathbf{x}_n+b)=1 \end{aligned} \] ### 求解 为了容易求解，可以把条件 \(\min_ny_n(\mathbf{w}^T\mathbf{x}_n+b)=1\) 换成 \[ y_n(\mathbf{w}^T\mathbf{x}_n+b)\ge1 \quad n=1,\dots,N \] 这个条件显然比原条件宽松，但是可以证明问题的最优解必然满足\(\min_ny_n(\mathbf{w}^T\mathbf{x}_n+b)=1\)。 用反证法。数据中同时存在正类和负类时必有\(\mathbf{w}\neq \mathbf{0}\)（否则\(b&gt;0\)且\(-b&gt;0\)，矛盾）。 假设\((b^*,\mathbf{w}^*)\)是问题的最优解，满足 \[ \rho^*=\min_ny_n(\mathbf{w}^T\mathbf{x}_n+b)&gt;1 \] 考虑\((b,\mathbf{w})=\frac{1}{\rho^*}(b^*,\mathbf{w}^*)\)。则\(\|\mathbf{w}\|=\frac{1}{\rho^*}\|\mathbf{w}^*\|&lt;\|\mathbf{w}^*\|\)，与\((b^*,\mathbf{w}^*)\)是最优解矛盾，得证。 那么问题变为 \[ \begin{aligned} \min_{b,\mathbf{w}}&amp;\quad\frac{1}{2}\mathbf{w}^T\mathbf{w}\\ s.t.&amp;\quad y_n(\mathbf{w}^T\mathbf{x}_n+b)\ge1\quad\text{for all } n \end{aligned} \label{problem1}\tag{1} \] 这就是svm的primal问题，是个二次规划的问题。 二次规划 标准形式： \[ \mathbf{u}^*\leftarrow \text{QP}(Q,\mathbf{p},A,\mathbf{c})\\ \begin{aligned} \min_{\mathbf{u}}&amp;\quad \frac{1}{2}\mathbf{u}^TQ\mathbf{u}+\mathbf{p}^T\mathbf{u}\\ s.t.&amp;\quad \mathbf{a}_m^T\mathbf{u}\ge c_m \quad m=1,\dots,M \end{aligned} \] \((\ref{problem1})\)可以写成标准形式： \[ \begin{aligned} \text{目标函数:}&amp;\quad\mathbf{u}=\begin{bmatrix} b\\ \mathbf{u} \end{bmatrix};Q=\begin{bmatrix} 0 &amp; \mathbf{0}_d^T\\ \mathbf{0}_d &amp; I_d \end{bmatrix};\mathbf{p}=\mathbf{0}_{d+1}\\ \text{约束:}&amp;\quad \mathbf{a}_n^T=y_n\begin{bmatrix} 1 &amp; \mathbf{x}_n^T \end{bmatrix};c_n=1;M=N \end{aligned} \] 其中\(d\)为\(\mathbf{x}\)的维度。 \(A=\begin{bmatrix}\mathbf{a}_1^T\\\vdots\\\mathbf{a}_N^T\end{bmatrix}\)与线性规划中的数据矩阵\(X\)相似，只不过每行多乘了\(y_n\)。 如果\(Q\)是半正定的，二次规划是凸的。这里的\(Q\)满足条件。 令问题的解为超平面\(g=(b^*,\mathbf{w}^*)\)，满足\(d(\mathbf{x}_n,g)=\frac{1}{\|\mathbf{w}^*\|}\)的数据点称为support vector(candidate)。把支持（或者叫支撑更合适）向量以外的数据点拿掉后，得到的最优分离超平面不会变。 hard margin的意思是超平面到两边margin的范围内没有数据点。 dual 现在考虑svm的对偶问题，因为在后面使用特征变换时会用到。 定义Lagrange函数： \[ L(b,\mathbf{w},\alpha)=\frac{1}{2}\mathbf{w}^T\mathbf{w}+\sum_{n=1}^N\alpha_n(1-y_n(\mathbf{w}^T\mathbf{x}_n+b)) \] 那么原问题和以下问题等价， \[ \min_{b,\mathbf{w}}\left(\max_{\text{all }\alpha_n\ge0}L(b,\mathbf{w},\alpha)\right) \] 因为如果\((b,\mathbf{w})\)不满足条件\(y_n(\mathbf{w}^T\mathbf{x}_n+b)\ge1\)，那么\(\max_{\alpha_n\ge0}\alpha_n(1-y_n(\mathbf{w}^T\mathbf{x}_n+b))\to\infty\)。反之，如果\((b,\mathbf{w})\)满足所有条件，\(\max_{\text{all }\alpha_n\ge0}\sum_{n=1}^N\alpha_n(1-y_n(\mathbf{w}^T\mathbf{x}_n+b))=0\)，即\(\max_{\text{all }\alpha_n\ge0}L(b,\mathbf{w},\alpha)=\frac{1}{2}\mathbf{w}^T\mathbf{w}\)。 对于任意满足要求的\(\alpha&#39;\)，因为\(\max_{\text{all }\alpha_n\ge0}L(b,\mathbf{w},\alpha)\ge L(b,\mathbf{w},\alpha&#39;)\)，所以 \[ \min_{b,\mathbf{w}}\left(\max_{\text{all }\alpha_n\ge0}L(b,\mathbf{w},\alpha)\right)\ge\min_{b,\mathbf{w}}L(b,\mathbf{w},\alpha&#39;) \] 由于上式对于任意\(\alpha&#39;\)都成立，自然应该大于等于其中的最大者，即： \[ \min_{b,\mathbf{w}}\left(\max_{\text{all }\alpha_n\ge0}L(b,\mathbf{w},\alpha)\right)\ge\max_{\text{all }\alpha&#39;_n\ge0}\min_{b,\mathbf{w}}L(b,\mathbf{w},\alpha&#39;) \] 右边换用符号\(\alpha\)，即 \[ \min_{b,\mathbf{w}}\left(\max_{\text{all }\alpha_n\ge0}L(b,\mathbf{w},\alpha)\right)\ge\max_{\text{all }\alpha_n\ge0}\left(\min_{b,\mathbf{w}}L(b,\mathbf{w},\alpha)\right) \] 上式左边为primal，右边为Lagrange dual。\(\ge\)表示weak duality。 对于二次规划，如果满足以下条件： primal是convex的 primal存在可行解 约束是线性的 那么\(\ge\)可用\(=\)取代，称为strong duality。 存在\((b,\mathbf{w},\alpha)\)，对于两边都是最优解。 求解dual内层： \[ \frac{\partial L}{\partial b}=0\Rightarrow\sum y_n\alpha_n=0\\ \] \[ \frac{\partial L}{\partial \mathbf{w}}=\mathbf{0}\Rightarrow\mathbf{w}=\sum \alpha_ny_n\mathbf{x}_n \label{condition1}\tag{2} \] 代入之后，dual变为： \[ \max_{\text{all }\alpha\ge0,\sum y_n\alpha_n=0,\mathbf{w}=\sum \alpha_ay_n\mathbf{x}_n}-\frac{1}{2}\left\|\sum_{n=1}^N\alpha_ny_n\mathbf{x}_n\right\|^2+\sum_{n=1}^N\alpha_n \] 从前面的推导中可见，最优解应满足的（必要）条件有： primal有可行解：\(y_n(\mathbf{w}^T\mathbf{x}_n+b)\ge1\) 对dual解的限制：\(\alpha_n\ge0\) dual内层最优条件：\(\sum y_n\alpha_n=0,\mathbf{w}=\sum \alpha_ay_n\mathbf{x}_n\) primal内层最优条件(complementary slackness)：\(\alpha_n\left(1-y_n(\mathbf{w}^T\mathbf{x}_n+b)\right)=0 \label{slackness}\tag{3}\) 这些称为KKT条件，这里也是充分条件（如何证明？） 将上式最大转为求最小，并展开第一项，得： \[ \begin{aligned} \min_{\alpha}&amp;\quad\frac{1}{2}\sum_{n=1}^N\sum_{m=1}^N\alpha_n\alpha_my_ny_m\mathbf{x}_n^T\mathbf{x}_m-\sum_{n=1}^N\alpha_n\\ s.t.&amp;\quad\sum_{n=1}^Ny_n\alpha_n=0\\ &amp;\quad\alpha_n\ge0\quad n=1,\dots,N \end{aligned} \] 这就是svm的dual问题。写成二次规划的标准形式： \[ \begin{aligned} \min_{\alpha}&amp;\quad\frac{1}{2}\alpha^TQ\alpha-\mathbf{1}_{N}^T\alpha\\ s.t.&amp;\quad A\alpha\ge\mathbf{0}_{N+2} \end{aligned} \] 其中 \[ Q_{n,m}=y_ny_m\mathbf{x}_n^T\mathbf{x}_m\\ A=\begin{bmatrix} \mathbf{y}^T\\ -\mathbf{y}^T\\ I_{N\times N} \end{bmatrix} \] \(A\)的前两行是因为\(\mathbf{y}^T\alpha=0\)等价于\(\mathbf{y}^T\alpha\ge0\)且\(\mathbf{y}^T\alpha\le0\)。 可以看到，\(Q=X_sX_s^T\)，其中\(X_s\)为带符号的数据矩阵： \[ \begin{aligned} X_s=\begin{bmatrix} y_1\mathbf{x}_1^T\\ \vdots\\ y_N\mathbf{x}_N^T \end{bmatrix} \end{aligned} \] 因此\(Q\)为半正定矩阵，这个二次规划问题是凸的。 要注意\(Q\)是\(N\times N\)的矩阵，\(N\)比较大时占用很大内存，SMO？ 恢复svm的解 现已求得最优解\(\alpha^*\)，\(\mathbf{w}^*\)由\((\ref{condition1})\)得出： \[ \mathbf{w}^*=\sum \alpha_n^*y_n\mathbf{x}_n \] \(\mathbf{w}^*\)是\(y_n\mathbf{x}_n\)的线性组合，这和感知机的学习算法的结果相似，称为\(\mathbf{w}\)由数据表征。 如果数据中同时有正类和负类，由前面说明过的\(\mathbf{w}^*\ne\mathbf{0}\)可知至少存在一个值\(\alpha^*_s\ne0\)。那么由\((\ref{slackness})\)得： \[ y_s(\mathbf{w}^{*T}\mathbf{x}_s+b^*)=1 \] 可知\((\mathbf{x}_s,y_s)\)属于support vector candidates。 求得： \[ \begin{aligned} b^*&amp;=y_s-\mathbf{w}^{*T}\mathbf{x}_s\\ &amp;=y_s-\sum_{n=1}^Ny_n\alpha_n^*\mathbf{x}_n^T\mathbf{x}_s \end{aligned} \] 容易看到，\(\mathbf{w}^{*}\)和\(b^*\)可以只用\(\alpha_n^*&gt;0\)对应的数据点得到，这些数据点称为support vector。 kernel trick 如果在\(\mathbf{x}\)空间中不是线性可分，考虑特征变换：\(\mathbf{z}=\Phi(\mathbf{x})\)，看在\(\mathbf{z}\)空间中能否做得更好。设\(\mathbf{x}\)的维度为\(d\)，\(\mathbf{z}\)的维度为\(\tilde{d}\)。计算\(Q_{n,m}=y_ny_m\mathbf{z}_n^T\mathbf{z}_m\)复杂度为\(O(\tilde{d})\)。需要降低计算\(\mathbf{z}_n^T\mathbf{z}_m=\Phi(\mathbf{x}_n)^T\Phi(\mathbf{x}_m)\)的复杂度。 二阶多项式变换：\(\Phi_2(\mathbf{x})=(1,x_1,x_2,\dots,x_d,x_1x_1,x_1x_2,\dots,x_dx_d)\)，这里为了后面推导方便，同时包含了\(x_ix_j\)和\(x_jx_i\)。 \[ \begin{aligned} \Phi_2(\mathbf{x})^T\Phi_2(\mathbf{x}&#39;)&amp;=1+\sum_{i=1}^dx_ix_i&#39;+\sum_{i=1}^d\sum_{j=1}^dx_ix_jx_i&#39;x_j&#39;\\ &amp;=1+\sum_{i=1}^dx_ix_i&#39;+\sum_{i=1}^dx_ix_i&#39;\sum_{j=1}^dx_jx_j&#39;\\ &amp;=1+\mathbf{x}^T\mathbf{x}&#39;+(\mathbf{x}^T\mathbf{x}&#39;)^2 \end{aligned} \] 只需要\(O(d)\)的复杂度，而\(\tilde{d}=O(d^2)\)。 kernel就是变换+内积。 kernel function：\(K_\Phi(\mathbf{x},\mathbf{x}&#39;)\triangleq \Phi(\mathbf{x})^T\Phi(\mathbf{x}&#39;)\)]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hoeffding不等式]]></title>
    <url>%2F2019%2F09%2F07%2FHoeffding%E4%B8%8D%E7%AD%89%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[Markov不等式：随机变量\(Z\geq 0\)。对于所有\(t\geq 0\)，有 \[ P(Z\geq t)\leq \frac{E[Z]}{t} \] 证明： \(Z\)的最小最大值分别用\(a,b\)表示 \[ \begin{aligned} P(Z\geq t)&amp;=\int_t^bp(z)dz\\ &amp;\leq \int_a^t\frac{z}{t}p(z)dz+\int_t^b\frac{z}{t}p(z)dz\\ &amp;=\frac{1}{t}\int_a^bzp(z)dz\\ &amp;=\frac{E[Z]}{t} \end{aligned} \] Chebyshev不等式：随机变量\(Z\)均值为\(\mu\)，方差为\(\sigma^2\)（有限），对于\(t\geq0\)，有 \[ P(|Z-\mu|\geq t)\leq \frac{\sigma^2}{t^2} \] 证明： \[ P(|Z-\mu|\geq t)=P((Z-\mu)^2\geq t^2) \] 然后应用Markov不等式。 令\(t=k\sigma\)，得另一种常见形式： \[ P(|Z-\mu|\geq k\sigma)\leq \frac{1}{k^2} \] 我们想要更强的bound。 定义随机变量\(Z\)的moment-generating function： \[ M_Z(s)\triangleq E[e^{sZ}] \] Chernoff bound：对于任意\(t&gt;0\)， \[ P(Z\geq t)\leq \min_{s&gt;0}e^{-st}M_Z(s) \] 证明： 对任意\(s&gt;0\)， \[ \begin{aligned} P(Z\geq t)&amp;=P(sZ\geq st)\\ &amp;=P(e^{sZ}\geq e^{st})\\ &amp;\leq e^{-st}E[e^{sZ}]\quad\text{Markov不等式}\\ &amp;=e^{-st}M_Z(s) \end{aligned} \] 因为对任意\(s&gt;0\)成立，自然小于最小的。 Hoeffding‘s lemma：随机变量\(X\)满足\(E[X]=0\)，且\(a\leq X\leq b\)1，那么对于任意\(s\)，有 \[ E[e^{sX}]\leq \exp\left(\frac{s^2(b-a)^2}{8}\right) \] 证明 Hoeffding不等式：\(X_1,\dots,X_n\)为独立随机变量，满足\(a_i\leq X_i\leq b_i\)。定义\(S_n=\sum_{i=1}^n X_i\)，那么对于\(t&gt;0\)，有 \[ P(S_n-E[S_n]\geq t)\leq e^{\frac{-2t^2}{\sum(b_i-a_i)^2}}\\ P(S_n-E[S_n]\leq -t)\leq e^{\frac{-2t^2}{\sum(b_i-a_i)^2}} \] 证明： 由Chernoff bound得 \[ \begin{aligned} P(S_n-E[S_n]\geq t)\leq\min_{s&gt;0}e^{-st}E\left[e^{s(S_n-E[S_n])}\right] \end{aligned} \] 并且 \[ \begin{aligned} E\left[e^{s(S_n-E[S_n])}\right]&amp;=\Pi_{i=1}^nE\left[e^{s(X_i-E[X_i])}\right]\quad \text{由于$X_i$相互独立}\\ &amp;\leq\Pi_{i=1}^n \exp\left(\frac{s^2(b_i-a_i)^2}{8}\right)\quad\text{由Hoeffding&#39;s lemma} \end{aligned} \] 然后只要求得 \[ \text{argmin}_{s&gt;0}-st+\frac{s^2}{8}\sum_{i=1}^n(b_i-a_i)^2 \] 代入就可得证。 由第一个公式及\(Z_i=-X_i\in[-b_i,-a_i]\)就得第二个公式。两个式子组合起来得到： \[ P(|S_n-E[S_n]|\geq t)\leq 2e^{\frac{-2t^2}{\sum(b_i-a_i)^2}} \] 如果\(X_i\)服从伯努利分布\(\text{Ber}(p)\)，那么\(a_i=0,b_i=1\)，\(S_n\)服从二项分布\(\text{ binom}(n,p)\)，\(E[s_n]=np\)。 \[ P(|S_n-np|&gt;t)=p\left(\left|\frac{S_n}{n}-p\right|&gt;\frac{t}{n}\right)\leq 2e^{\frac{-2t^2}{n}} \] 令\(\frac{t}{n}=\epsilon\)，得： \[ p\left(\left|\frac{1}{n}\sum_{i=1}^nX_i-p\right|&gt;\epsilon\right)\leq 2e^{-2n\epsilon^2} \] 由\(E[X]=0\)可得\(a\leq 0\leq b\)↩︎]]></content>
      <categories>
        <category>数学</category>
        <category>概率统计</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[感知机]]></title>
    <url>%2F2019%2F08%2F31%2F%E6%84%9F%E7%9F%A5%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[正类用+1表示，负类用-1表示，希望学得 \[ f(\mathbf{x})=\text{sign}(\mathbf{w}\cdot\mathbf{x}+b) \] \(\mathbf{w}\cdot\mathbf{x}+b=0\)是一个超平面，希望它能完全分隔正类/负类。这里假设这是可以做到的，称为线性可分。 损失函数 任一点\(\mathbf{x}_0\)到超平面的距离为： \[ \frac{|\mathbf{w}\cdot\mathbf{x}_0+b|}{\|\mathbf{w}\|} \] 对于误分类的数据\((\mathbf{x}_i,y_i)\)来说， \[ -y_i(\mathbf{w}\cdot\mathbf{x}_i+b)&gt;0 \] 因此，误分类点\(\mathbf{x}_i\)到超平面的距离为 \[ \frac{-y_i(\mathbf{w}\cdot\mathbf{x}_i+b)}{\|\mathbf{w}\|} \] 假设误分类点的集合为\(M\)，那么总距离为 \[ -\frac{1}{\|\mathbf{w}\|}\sum_{\mathbf{x}_i\in M}y_i(\mathbf{w}\cdot\mathbf{x}_i+b) \] 不考虑\(\frac{1}{\|\mathbf{w}\|}\)，就得到损失函数： \[ L(\mathbf{w},b)=-\sum_{\mathbf{x}_i\in M}y_i(\mathbf{w}\cdot\mathbf{x}_i+b) \] 令\(x_0=1,w_0=b\)，可以写成： \[ -\sum_{\mathbf{x}_i\in M}y_i\mathbf{w}\cdot\mathbf{x}_i \] 求解 接下来用随机梯度下降求解，即每次只选取一个误分类点使用梯度下降。假设选取的误分类点为\((\mathbf{x}_i,y_i)\)， \[ \mathbf{w}\leftarrow\mathbf{w}+\eta y_i\mathbf{x}_i \] 其中\(\eta\)为步长（学习率）。 更新之后 \[ \begin{aligned} y_i(\mathbf{w}+\eta y_i\mathbf{x}_i)\cdot \mathbf{x}_i&amp;=y_i\mathbf{w}\cdot \mathbf{x}_i+\eta\mathbf{x}_i\cdot \mathbf{x}_i\\ &amp;\geq y_i\mathbf{w}\cdot \mathbf{x}_i \end{aligned} \] 所以更新后更接近正确分类（\(y_i\mathbf{w}\cdot \mathbf{x}_i&gt;0\)）。 算法收敛性 从上面的过程可以看出，在纠正一个误分类点时，可能会使原来正确分类的点被误分类。如何证明算法收敛？ 因为线性可分，存在\(\|\mathbf{w}_f\|=1\)，使得 \[ \forall i\quad y_i\mathbf{w}_f\cdot\mathbf{x}_i\geq\min_n y_n\mathbf{w}_f\cdot\mathbf{x}_n\triangleq\gamma&gt;0 \] \(\mathbf{w}_k\)表示第\(k\)轮更新后的参数，\(\mathbf{w}_0\)设为全0向量。 假设\((\mathbf{x}_i,y_i)\)是第\(k\)轮更新中被选中的误分类点 \[ \begin{aligned} \mathbf{w}_f\cdot\mathbf{w}_{k}&amp;=\mathbf{w}_f\cdot(\mathbf{w}_{k-1}+\eta y_i\mathbf{x}_i)\\ &amp;\geq \mathbf{w}_f\cdot\mathbf{w}_{k-1}+\eta\gamma\\ &amp;\geq\dots\\ &amp;\geq k\eta\gamma \end{aligned} \] 还要看\(\|\mathbf{w}_k\|\)的增长速度， \[ \begin{aligned} \|\mathbf{w}_k\|^2&amp;=\|\mathbf{w}_{k-1}\|^2+2\eta y_i\mathbf{w}_{k-1}\cdot\mathbf{x}_{i}+\eta^2\|\mathbf{x}_i\|^2\\ &amp;\leq\|\mathbf{w}_{k-1}\|^2+\eta^2\|\mathbf{x}_i\|^2\quad(由误分类条件)\\ &amp;\leq\|\mathbf{w}_{k-1}\|^2+\eta^2R^2\quad(R^2\triangleq\max_i\|\mathbf{x}_i\|^2)\\ &amp;\leq\dots\\ &amp;\leq k\eta^2R^2 \end{aligned} \] 那么由 \[ \frac{k\eta\gamma}{\sqrt{k}\eta R}\leq\mathbf{w}_f\cdot\frac{\mathbf{w}_k}{\|\mathbf{w}_k\|}\leq 1 \] 得 \[ k\leq \left(\frac{R}{\gamma}\right)^2 \] 即迭代次数有上限，算法会收敛。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[坐标系转换]]></title>
    <url>%2F2019%2F08%2F25%2F%E5%9D%90%E6%A0%87%E7%B3%BB%E8%BD%AC%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[方向余弦矩阵（DCM） \(\mathbf{r}_A,\mathbf{r}_B\)分别为向量\(\mathbf{r}\)在坐标系\(A，B\)中的坐标，由 \[ \mathbf{r}= \begin{pmatrix}\mathbf{i}_B &amp; \mathbf{j}_B &amp; \mathbf{k}_B \end{pmatrix}\mathbf{r}_B=\begin{pmatrix}\mathbf{i}_A &amp; \mathbf{j}_A &amp; \mathbf{k}_A \end{pmatrix}\mathbf{r}_A \] 得： \[ \begin{aligned} \mathbf{r}_B&amp;=\begin{pmatrix}\mathbf{i}_B^\intercal\\ \mathbf{j}_B^\intercal\\ \mathbf{k}_B^\intercal \end{pmatrix}\begin{pmatrix}\mathbf{i}_A &amp; \mathbf{j}_A &amp; \mathbf{k}_A \end{pmatrix}\mathbf{r}_A\\ &amp;\triangleq R_{BA}\mathbf{r}_A \end{aligned} \] \(R_{BA}\)表示从\(A\)到\(B\)的坐标转换矩阵。 由\(R_{BA}\)的定义还可得基变换： \[ \begin{pmatrix}\mathbf{i}_A &amp; \mathbf{j}_A &amp; \mathbf{k}_A \end{pmatrix}=\begin{pmatrix}\mathbf{i}_B &amp; \mathbf{j}_B &amp; \mathbf{k}_B \end{pmatrix}R_{BA} \] 欧拉角 intrinsic rotation是指绕当前坐标系（而不是某个固定坐标系）的轴转动。 \(A\)绕某轴逆时针旋转\(\theta\)，得到\(B\)。对于\(R_{BA}\)，根据定义得出如下结果1： 绕\(X\)轴旋转 \[ R_x= \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; c_\theta &amp; s_\theta \\ 0 &amp; -s_\theta &amp; c_\theta \end{bmatrix} \] 绕\(Y\)轴旋转 \[ R_y= \begin{bmatrix} c_\theta &amp; 0 &amp; -s_\theta \\ 0 &amp; 1 &amp; 0\\ s_\theta &amp; 0 &amp; c_\theta \end{bmatrix} \] 绕\(Z\)轴旋转 \[ R_z= \begin{bmatrix} c_\theta &amp; s_\theta &amp; 0\\ -s_\theta &amp; c_\theta &amp; 0\\ 0 &amp; 0 &amp; 1 \end{bmatrix} \] 如果按\(Z\rightarrow Y\rightarrow X\)的顺序转动， \[ R_{BA}=R_xR_yR_z=\begin{bmatrix} c_yc_z &amp; c_ys_z &amp; -s_y \\ s_xs_yc_z-c_xs_z &amp; s_xs_ys_z+c_xc_z &amp; s_xc_y \\ c_xs_yc_z+s_xs_z &amp; c_xs_ys_z-s_xc_z &amp; c_xc_y \end{bmatrix} \] 可得欧拉角和DCM的转换关系： \[ \text{tan}(\theta_z)=\frac {R_{12}} {R_{11}}\\ \text{sin}(\theta_y)=-R_{13}\\ \text{tan}(\theta_x)=\frac {R_{23}} {R_{33}} \] 若是小角度转动， \[ \begin{equation} R=\begin{bmatrix} 1 &amp; \theta_z &amp; -\theta_y\\ -\theta_z &amp; 1 &amp; \theta_x\\ \theta_y &amp; -\theta_x &amp; 1 \end{bmatrix}=I-[\theta]_\times \end{equation} \] 可见小角度转动与转动顺序无关（即只与绕XYZ各轴转过的角度有关。如果只有俩轴，比如Z-&gt;X-&gt;Z，那么未出现的轴角度为0，出现两次的轴的角度相加）。 Extrinsic rotation 设固定坐标系为\(A\)。坐标系\(B\)绕\(A\)的\(\mathbf{u}\)轴转动角度\(\phi\)，这个转动用\(R_{\phi \mathbf{u}}\)表示，得坐标系\(C\)。求\(R_{CA}\)。 首先 \[ R_{CA}=R_{BD} \] 其中\(D\)为\(A\)绕\(\mathbf{u}\)轴转动角度\(-\phi\)得到。这个结论对于二维转动很直观，对于三维其实也容易看出。按定义，只要证明两对坐标系的基的内积相等即可。把基分解到平行于转轴和垂直于转轴。平行部分的内积显然不变，垂直部分由二维情况可知也不变，且平行于垂直部分内积为0。因此得证。 \[ R_{BD}=R_{BA}R_{AD}=R_{BA}R_{DA}^{-1}=R_{BA}R_{-\phi \mathbf{u}}^{-1}=R_{BA}R_{\phi \mathbf{u}} \] 因此得到结论，绕固定坐标系的轴转动，\(R\)是乘在右边的。 四元数 \[ \mathbf{x}_G=\mathbf{q}_{GL}\otimes \mathbf{x}_L\otimes\mathbf{q}^*_{GL} \] 其中G表示global，或者n系；L表示local，或者b系。 这样定义是为了和旋转三维向量的公式形式保持一致。 运动方程 四元数 如果是在L系中表达扰动（即上述的intrinsic rotation）， \[ q(t+\Delta t)=q(t)\otimes\Delta q_L=q(t)\otimes\text{Exp}(\Delta\phi_L) \] \[ \begin{aligned} \dot{q}&amp;\triangleq \lim_{\Delta t\to 0}\frac{q(t+\Delta t)-q(t)}{\Delta t}\\ &amp;=\lim_{\Delta t\to 0}\frac{q\otimes\Delta q_L-q}{\Delta t}\\ &amp;=\lim_{\Delta t\to 0}\frac{q\otimes\left(\begin{bmatrix}1\\\Delta\phi_L/2\end{bmatrix}-\begin{bmatrix}1\\0\end{bmatrix}\right)}{\Delta t}\\ &amp;=\lim_{\Delta t\to 0}\frac{q\otimes\begin{bmatrix}0\\\Delta\phi_L/2\end{bmatrix}}{\Delta t}\\ &amp;=\frac{1}{2}q\otimes \omega_L \end{aligned} \] 其中\(\text{Exp}(\Delta\phi_L)\)用了小角度近似。 \(s_\theta\)放置位置记忆方法：放在转动轴的上一列（循环）。↩︎]]></content>
      <tags>
        <tag>四元数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[四元数]]></title>
    <url>%2F2019%2F08%2F17%2F%E5%9B%9B%E5%85%83%E6%95%B0%2F</url>
    <content type="text"><![CDATA[定义 \[ Q = a+bi+cj+dk \] 其中\({i,j,k}\)是虚数单元，满足： \[ i^2=j^2=k^2=ijk=-1 \] \(Q=a\)称为实四元数，\(Q=bi+cj+dk\)称为虚四元数。 实部+虚部的表示方式有时不方便，只要遵循虚部的运算规则，就可以写成标量+向量的形式： \[ Q=q_w+q_xi+q_yj+q_zk\quad \Leftrightarrow\quad Q=q_w+\mathbf{q}_v \] 下面将四元数\(Q\)写成四维向量\(\mathbf{q}\)的形式： \[ \mathbf{q}\triangleq \begin{bmatrix} q_w\\ \mathbf{q}_v \end{bmatrix}= \begin{bmatrix} q_w\\ q_x\\ q_y\\ q_z \end{bmatrix} \] 这样就可以用矩阵来进行有关四元数的运算。 主要性质 乘法 四元数的乘法只要按定义来进行就行了，用标量和向量的形式来表示是比较方便的： \[ \mathbf{p}\otimes\mathbf{q}= \begin{bmatrix} p_wq_w-\mathbf{p}_v^\top\mathbf{q}_v\\ p_w\mathbf{q}_v+q_w\mathbf{p}_v+\mathbf{p}_v\times\mathbf{q}_v \end{bmatrix} \] 由于结果的向量部分有个叉积，而叉积不满足交换律，因此四元数乘法一般也不满足交换律，即： \[ \mathbf{p}\otimes\mathbf{q}\neq\mathbf{q}\otimes\mathbf{p} \] 但是乘法满足结合律（暴力验证）： \[ (\mathbf{p}\otimes\mathbf{q})\otimes\mathbf{r}=\mathbf{p}\otimes(\mathbf{q}\otimes\mathbf{r}) \] 四元数乘法可以写成矩阵乘法的形式： \[ \mathbf{q_1}\otimes\mathbf{q_2}=[\mathbf{q_1}]_L\mathbf{q_2}\\ \mathbf{q_1}\otimes\mathbf{q_2}=[\mathbf{q_2}]_R\mathbf{q_1} \] 其中 \[ [\mathbf{q}]_L=q_w\mathbf{I}+ \begin{bmatrix} 0 &amp; -\mathbf{q}_v^\top\\ \mathbf{q}_v &amp; [\mathbf{q}_v]_{\times} \end{bmatrix} \] \[ [\mathbf{q}]_R=q_w\mathbf{I}+ \begin{bmatrix} 0 &amp; -\mathbf{q}_v^\top\\ \mathbf{q}_v &amp; -[\mathbf{q}_v]_{\times} \end{bmatrix} \] 这里用到了反对称矩阵： \[ [\mathbf{a}]_{\times}\triangleq \begin{bmatrix} 0 &amp; -a_z &amp; a_y\\ a_z &amp; 0 &amp; -a_x\\ -a_y &amp; a_x &amp; 0 \end{bmatrix} \] 这个矩阵和叉积有关： \[ [\mathbf{a}]_{\times}\mathbf{b}=\mathbf{a}\times\mathbf{b} \] \[ [\mathbf{q}]_L[\mathbf{q}]_L^T=[\mathbf{q}]_R[\mathbf{q}]_R^T=\|\mathbf{q}\|^2I \] 即如果\(\mathbf{q}\)是单位四元数，那么这两个矩阵是正交矩阵。 由于 \[ \begin{aligned} \mathbf{q}\otimes\mathbf{x}\otimes\mathbf{p}&amp;=(\mathbf{q}\otimes\mathbf{x})\otimes\mathbf{p}=[\mathbf{p}]_R[\mathbf{q}]_L\mathbf{x}\\ &amp;=\mathbf{q}\otimes(\mathbf{x}\otimes\mathbf{p})=[\mathbf{q}]_L[\mathbf{p}]_R\mathbf{x} \end{aligned} \] 可见L,R矩阵满足交换律： \[ [\mathbf{p}]_R[\mathbf{q}]_L=[\mathbf{q}]_L[\mathbf{p}]_R \] identity 幺元\(\mathbf{q_1}\)满足： \[ \mathbf{q_1}\otimes\mathbf{q}=\mathbf{q}\otimes\mathbf{q_1}=\mathbf{q} \] 它就是实数1： \[ \mathbf{q_1}=1= \begin{bmatrix} 1\\ \mathbf{0}_v \end{bmatrix} \] 共轭 和复数类似，共轭定义为： \[ \mathbf{q}^*\triangleq q_w-\mathbf{q}_v= \begin{bmatrix} q_w\\ -\mathbf{q}_v \end{bmatrix} \] 满足： \[ \mathbf{q}\otimes\mathbf{q}^*=\mathbf{q}^*\otimes\mathbf{q}=q_w^2+q_x^2+q_y^2+q_z^2 \] \[ (\mathbf{p}\otimes\mathbf{q})^*=\mathbf{q}^*\otimes\mathbf{p}^* \] norm 范数定义为： \[ \|\mathbf{q}\| \triangleq \sqrt{\mathbf{q}\otimes\mathbf{q}^*}=\sqrt{q_w^2+q_x^2+q_y^2+q_z^2} \] 满足： \[ \|\mathbf{p}\otimes\mathbf{q}\|=\|\mathbf{p}\|\|\mathbf{q}\| \] 逆 逆定义为： \[ \mathbf{q}\otimes\mathbf{q}^{-1}=\mathbf{q}^{-1}\otimes\mathbf{q}=\mathbf{q}_1 \] 显然， \[ \mathbf{q}^{-1}=\frac{\mathbf{q}^*}{\|\mathbf{q}\|^2} \] 单位四元数 或者叫归一化的四元数定义为\(\|\mathbf{q}\|=1\)，因此 \[ \mathbf{q}^{-1}=\mathbf{q}^* \] 单位四元数可以写成： \[ \mathbf{q}= \begin{bmatrix} \cos\theta\\ \mathbf{u}\sin\theta \end{bmatrix} \] 其中\(\|\mathbf{u}\|=1\)。 附加性质 虚四元数的乘法 \[ \mathbf{q}_v\otimes\mathbf{q}_v=-\mathbf{q}_v^\top\mathbf{q}_v=-\|\mathbf{q}_v\|^2 \] 对于单位虚四元数\(\|\mathbf{u}\|=1\)，因此： \[ \mathbf{u}\otimes \mathbf{u}=-1 \] 与虚数\(i\cdot i=-1\)类似。 虚四元数的指数函数 与实数类似，根据级数展开来定义： \[ e^{\mathbf{q}}\triangleq \sum_{0}^{\infty}\frac{1}{k!}\mathbf{q}^k \] 对于虚四元数\(\mathbf{v}=\mathbf{u}\theta\)，其中\(\|\mathbf{u}\|=1\)，有： \[ e^{\mathbf{v}}=e^{\mathbf{u}\theta}=\cos\theta+\mathbf{u}\sin\theta= \begin{bmatrix} \cos\theta\\ \mathbf{u}\sin\theta \end{bmatrix} \] 是虚数的欧拉公式的扩展。 注意\(\|e^{\mathbf{v}}\|=1\)，因此虚四元数的指数函数为单位四元数。 一般四元数的指数函数 由于当其中一个四元数为实数时，四元数乘法满足交换律，因此： \[ e^{\mathbf{q}}=e^{q_w+\mathbf{q}_v}=e^{q_w}e^{\mathbf{q}_v} \] 单位四元数的对数 四元数的对数用指数来定义，如果\(\|\mathbf{q}\|=1\)， \[ \log \mathbf{q}=\log(\cos\theta+\mathbf{u}\sin\theta)=\log(e^{\mathbf{u}\theta})=\mathbf{u}\theta \] 一般四元数的对数 \[ \log\mathbf{q}=\log(\|\mathbf{q}\|\frac{\mathbf{q}}{\|\mathbf{q}\|})=\log\|\mathbf{q}\|+\mathbf{u}\theta \] Exponential forms of the type \(\mathbf{q}^t\) 对于\(t\in \mathbb{R}\)， \[ \mathbf{q}^t=\exp(\log(\mathbf{q}^t))=\exp(t\log(\mathbf{q})) \] 如果\(\|\mathbf{q}\|=1\)，写成\(\mathbf{q}=\begin{bmatrix}\cos\theta, &amp; \mathbf{u}\sin\theta\end{bmatrix}\)，因此\(\log(\mathbf{q})=\mathbf{u}\theta\)，那么 \[ \mathbf{q}^t=\exp(t\mathbf{u}\theta)= \begin{bmatrix} \cos t\theta\\ \mathbf{u}\sin t\theta \end{bmatrix} \] 参考资料 Quaternion kinematics for the error-state Kalman filter]]></content>
      <categories>
        <category>数学</category>
      </categories>
      <tags>
        <tag>四元数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[奇异值分解]]></title>
    <url>%2F2019%2F08%2F10%2F%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[对称矩阵的特征值分解： \[ A=VDV^T \] 其中\(D=\text{diag}(\delta_1,\dots,\delta_n)\)。特征向量\(\{v_i\}\)是正交归一的，作为空间的一组基很方便。 从线性变换的角度，\(A\)将\(\rm{R^n}\)中的向量\(x\)变换到同一个空间。对\(x\)做展开： \[ x = \sum_{i=1}^nc_iv_i \] 那么 \[ Ax = \sum_{i=1}^nc_i\delta_iv_i \] 即会根据特征值大小，对分量放大或缩小。 SVD 如果\(A\)是\(m\times n\)的矩阵，\(A\)将\(\rm{R^n}\)空间的向量\(x\)变换到\(\rm{R^m}\)，又该如何选择定义域和值域中的基呢？ \(\{v_1,v_2,\dots,v_n\}\)表示\(\rm{R^n}\)的一组正交归一基，其中\(\{v_1,v_2,\dots,v_r\}\)张成\(A\)的行空间（\(r\)为\(A\)的秩），那么 \[ Av_i\neq \mathbf{0} \quad 1\leq i\leq r \] \(\{Av_1,Av_2,\dots,Av_r\}\)也是线性无关的，因为如果存在不全为0的数\(c_i\)，使得 \[ \sum_{i=1}^rc_i(Av_i)=\sum_{i=1}^rA(c_iv_i)=\mathbf{0} \] 说明\(\sum_{i=1}^rc_iv_i\)在\(A\)的nullspace，这与\(v_i\)在行空间中矛盾。 \(\{u_1,u_2,\dots,u_m\}\)表示\(\rm{R^m}\)的一组归一化的基。将矩阵\(A\)表示为如下形式并不难 \[ A=U\Sigma V^T \] 其中\(\Sigma\)为\(m\times n\)的矩阵，非零元只在主对角线上。只需让\(Av_i=\sigma_iu_i\)（\(v_i,u_i\)分别为\(V,U\)的第\(i\)列）。如上，虽然\(\{u_i=\frac{Av_i}{\sigma_i}, 1\leq i\leq r\}\)是线性无关的，但不能保证是正交的。 为了保证正交，我们不能任意选取\(\rm{R^n}\)的正交归一基作为\(V\)。 满足要求的\(\{v_i\}\)是\(A^TA\)的特征向量： \[ A^TAv_i=\lambda_iv_i \] 那么 \[ (Av_i)^T(Av_j)=v_i^TA^TAv_j=v_i^T(\lambda_jv_j)=\lambda_jv_i^Tv_j \] 因此\(Av_i\)与\(Av_j\)正交。 上式中令\(i=j\)，得 \[ \lambda_i=\|Av_i\|^2\geq 0 \] 排列特征值，使得： \[ \lambda_1\geq\lambda_2\geq\cdots\geq\lambda_r&gt;0\\ \lambda_{i}=0 \quad i&gt;r \] 由于\(i&gt;r\)时，\(\|Av_i\|=0\)，得到 \[ Av_i=\mathbf{0}\quad i&gt;r \] 所以\(\{v_i\mid 1\le i\le r\}\)在\(A\)的行空间里。又因为它们相互正交（则线性无关），所以\(r\)是\(A\)的秩。 为了得到\(\rm{R^m}\)中的一组基，归一化\(Av_i\)， \[ u_i=\frac{Av_i}{\|Av_i\|}=\frac{Av_i}{\sqrt{\lambda_i}}\quad 1\leq i\leq r \] 称\(\sigma_i=\sqrt{\lambda_i}\)为奇异值。 如果\(r&lt;m\)，继续补全基中还需的向量（按Gram–Schmidt过程来就行了）。 另外，可得\(AA^Tu_i=\lambda_iu_i\)，即\(\{u_i\}\)是\(AA^T\)的特征向量。 令\(U_r=[u_1,\dots,u_r],\Sigma_r=diag(\sigma_1,\dots,\sigma_r),V_r=[v_1,\dots,v_r]\)，显然有： \[ A=U_r\Sigma_rV_r=\sum_{i=1}^r\sigma_iu_iv_i^T \] 即\(r\)个秩为1的矩阵之和。 应用 最小二乘 \[ \begin{align} \|Ax-b\|^2&amp;=\|U^T(Ax-b)\|\quad(U\text{是正交矩阵})\\ &amp;=\|\Sigma V^Tx-U^Tb\|^2\\ &amp;=\sum_{i=1}^r(\sigma_iz_i-u_i^Tb)^2+\sum_{i=r+1}^m(u_i^Tb)^2\quad(z=V^Tx) \end{align} \] 为了使上式最小， \[ z_i=\frac{u_i^Tb}{\sigma_i}\quad i\le r\\ z_i=任意\quad r&lt;i\le n \] 最后得\(x=Vz\)。\(m\ge n\)且\(r=n\)时，与法方程的解一致。]]></content>
      <categories>
        <category>数学</category>
        <category>线性代数</category>
      </categories>
      <tags>
        <tag>SVD</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[实对称矩阵性质]]></title>
    <url>%2F2019%2F08%2F10%2F%E5%AE%9E%E5%AF%B9%E7%A7%B0%E7%9F%A9%E9%98%B5%E6%80%A7%E8%B4%A8%2F</url>
    <content type="text"><![CDATA[特征值都是实数 证明： \[ \tag{1}Ax=\lambda x \] 两边左乘\(x\)的共轭转置： \[ \tag{2}\bar{x}^TAx=\lambda\bar{x}^Tx \] 对\((1)\)式取共轭，得： \[ \bar{A}\bar{x}=\bar{\lambda}\bar{x} \] 由于\(A\)是实矩阵，\(\bar{A}=A\)，因此： \[ A\bar{x}=\bar{\lambda}\bar{x} \] 两边取转置： \[ \begin{aligned} \bar{x}^TA^T&amp;=\bar{x}^TA\quad(\text{$A$是对称阵})\\ &amp;=\bar{\lambda}\bar{x}^T \end{aligned} \] 右乘\(x\)，得： \[ \bar{x}^TAx=\bar{\lambda}\bar{x}^Tx \] 与\((2)\)式比较得： \[ \lambda\bar{x}^Tx=\bar{\lambda}\bar{x}^Tx \] 由于\(x\neq \mathbf{0}\)，那么\(\bar{x}^Tx=\sum\|x_i\|^2\neq 0\)，因此\(\lambda=\bar{\lambda}\)，即特征值为实数。 不同特征值对应的特征向量正交 证明： \[ \begin{aligned} x_2^TAx_1&amp;=x_2^T(Ax_1)\\ &amp;=\lambda_1x_2^Tx_1 \end{aligned} \] \[ \begin{aligned} x_2^TAx_1&amp;=(x_2^TA)x_1\\ &amp;=(A^Tx_2)^Tx_1\\ &amp;=(Ax_2)^Tx_1 \quad(\text{$A$是对称阵})\\ &amp;=\lambda_2x_2^Tx_1 \end{aligned} \] 比较两式，得： \[ \lambda_1x_2^Tx_1=\lambda_2x_2^Tx_1 \] 由于\(\lambda_1\neq\lambda_2\)，因此\(x_2^Tx_1=0\)。 对角化 由于不同特征值对应的特征向量正交，而同一个特征值如果有多重特征向量，可以用Gram–Schmidt正交化，因此一般的对角化\(A=PDP^{-1}\)中，可以通过选取合适的特征向量，使得\(P\)成为正交矩阵，因此可以对实对称阵做正交对角化： \[ A=PDP^T \]]]></content>
      <categories>
        <category>数学</category>
        <category>线性代数</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[线性筛法]]></title>
    <url>%2F2019%2F08%2F10%2F%E7%BA%BF%E6%80%A7%E7%AD%9B%E6%B3%95%2F</url>
    <content type="text"><![CDATA[线性筛法不仅是线性的（每个数只筛一次），还能得到每个数的最小素因子，而有了这，因式分解就很容易了。 对于合数\(x\)，令\(p_x\)表示\(x\)的最小素因子，则 \[ x=i*p_x \] 每个合数就是在\(i=x/p_x\)的时候筛掉的，也就是尽可能晚的时候。 1234567891011121314151617181920const int N=1e5;int minPrimeFactor[N+1];vector&lt;int&gt; primes;void linearSieve()&#123; for (int i=2; i&lt;=N; ++i) &#123; if (minPrimeFactor[i] == 0) &#123; minPrimeFactor[i] = i; primes.push_back(i); &#125; for (auto p : primes) &#123; if (p &gt; minPrimeFactor[i] || i * p &gt; N) break; minPrimeFactor[i*p] = p; &#125; &#125; &#125; 参考资料 hackerrank David Gries, Jayadev Misra. A Linear Sieve Algorithm for Finding Prime Numbers [1978]]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Hough变换]]></title>
    <url>%2F2019%2F08%2F10%2FHough%E5%8F%98%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[检测直线 给定二值图像中的\(n\)个点，要找到图像中的直线。一种方法是枚举所有可能的\(n*(n-1)/2\)条直线，然后检测每个点是否足够靠近线。如果足够靠近，就给这条线投一票。最后票数多的直线很可能是真正的直线。这样的复杂度是\(n^3\)的，对于一般的图片大小并不实用。 参数空间方法 Hough变换的思想最初是Hough在一篇专利中提出的。 考虑\(xy\)平面上的一点\((x_i,y_i)\)，假设通过这个点的某条直线的斜率为\(a\)，截距为\(b\)，那么\(y_i = ax_i+b\)。将式子写成\(b=-x_ia+y_i\)，即通过给定点的直线的斜率和截距满足一定关系。在\(ab\)平面（参数空间），这是一条直线。另一个点\((x_j,y_j)\)在参数空间也有一条关联的直线。如果这两条直线不平行，那么会交于一点\((a&#39;,b&#39;)\)。\((a&#39;,b&#39;)\)既是通过\((x_i,y_i)\)的某条直线的参数，又是通过\((x_j,y_j)\)的某条直线的参数，显然这条直线就是连接\((x_i,y_i)\)，\((x_j,y_j)\)的直线。 以上的方案有一点缺陷。我们知道只要\((x_i,y_i)\)，\((x_j,y_j)\)不是同一点，总能唯一确定一条直线。然而参数空间却有可能无法表示对应的参数值。也就是参数空间的两条直线平行的时候，无法求得参数。此时两条直线斜率相同：\(-x_i=-x_j\)。也就是当两点的\(x\)坐标相同时，连线是一条竖直的线，斜率无穷大，无法在参数空间表示。 法线表示 针对上述问题，我们现在用的Hough变换版本是Hart（和Duda）1972年在《Use of the Hough transformation to detect lines and curves in pictures》这篇论文中提出的。1 它利用的是直线的另一种表示：法线表示。 \[ x\cos\theta+y\sin\theta = \rho \] 其中\(\theta\)是直线的法线和\(x\)轴的夹角，\(\rho\)是原点到直线的距离（根据对\(\theta\)取值范围的规定，可能为负）。把上式写成向量\((x,y)\)与单位法向量\((\cos\theta,\sin\theta)\)的点积，就很显然。从几何上容易看出，\(\rho\)的最大取值为图像的对角线长度，因此不会有斜率-截距表示法中的无穷大问题。 图像中的一个点关联着\(\rho\theta\)参数空间中的一条正弦曲线。一条直线上的点对应的正弦曲线族在\(\rho\theta\)空间中会交于同一点。如果每个\((\theta,\rho)\)关联着一个累加器，其数值是参数空间中所有正弦曲线通过它的次数，那么某条直线对应的累积器数值就是直线上点的个数。我们无法枚举无限多的\((\theta,\rho)\)，Hough变换的做法是将\(\rho\theta\)空间按一定的分辨率网格化。对于图像中每个点，按设定的分辨率枚举所有的\(\theta\)，按\(\rho = x\cos\theta+y\sin\theta\) 得到对应的\(\rho\)，根据设定的\(\rho\)的分辨率得到对应网格，其累加器加一。最后把数值大的累加器对应的参数认定为直线。 opencv中的实现 opencv中\(\theta\)的取值范围为0到180度，那么\(\rho\)是有可能为负的。其范围为\(-D\leq\rho\leq D\)，其中\(D\)为对角线长。 实现在hough.cpp的HoughLinesStandard函数中。 123int max_rho = width + height;int min_rho = -max_rho;int numrho = cvRound(((max_rho - min_rho) + 1) / rho); 可以看到它取的\(\rho\)的最大值为宽加高，保证了大于对角线长。 123456789101112// stage 1. fill accumulatorfor( i = 0; i &lt; height; i++ ) for( j = 0; j &lt; width; j++ ) &#123; if( image[i * step + j] != 0 ) for(int n = 0; n &lt; numangle; n++ ) &#123; int r = cvRound( j * tabCos[n] + i * tabSin[n] ); r += (numrho - 1) / 2; accum[(n+1) * (numrho+2) + r+1]++; &#125; &#125; int r = cvRound( j * tabCos[n] + i * tabSin[n] );表示\(r = \frac{x\cos\theta+y\sin\theta}{\Delta\rho}\)，其中\(\Delta \rho\)为\(\rho\)的分辨率。这里\(r\)取值范围在\(-\frac{D}{\Delta \rho}\)和\(\frac{D}{\Delta \rho}\)之间，r += (numrho - 1) / 2;是为了把值shift到非负范围才能作为累加器的列索引。（累加器的行索引为\(\theta\)，列索引为\(\rho\)）。 下面的代码则是相应地还原回真正的\(\rho\)。 123int n = cvFloor(idx*scale) - 1;int r = idx - (n+1)*(numrho+2) - 1;line.rho = (r - (numrho - 1)*0.5f) * rho; 其他实现细节 在累加器矩阵中只会保留局部极大值，也就是比上下左右的累积器的值都要大才有可能输出，这样可以去掉非常接近的线。 结果是按累加器的值从大到小排序的。 指定\(\theta\)范围 在做跟踪的时候，可能只需要某个范围的\(\theta\)中做检测。Houghlines中可以指定最小和最大的\(\theta\)。如果是以两个点表示的直线，先要得到\(\theta\)值。由 \[ x_1\cos(\theta)+y_1\sin(\theta) = x_2\cos(\theta)+y_2\sin(\theta) = \rho \] 得： \[ \tan\theta = \frac{x_1-x_2}{y_2-y_1} \] 又因为在opencv中\(\theta\in [0,\pi]\)，因此 \[ \theta = \begin{cases} \arctan(\frac{x_1-x_2}{y_2-y_1}) , &amp; \text{if $\arctan(\frac{x_1-x_2}{y_2-y_1})\geq 0$ } \\ \pi - \arctan(\frac{x_1-x_2}{y_2-y_1}), &amp; \text{else} \end{cases} \] 直线和\(\theta\)对应关系如下图，可见在竖直线时\(\theta\)不连续变化。 有兴趣的可以看Hart写的《How the Hough transform was invented》了解这段历史↩︎]]></content>
      <categories>
        <category>计算机视觉</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Iterative Closest Point]]></title>
    <url>%2F2019%2F08%2F10%2FIterative%20Closest%20Point%2F</url>
    <content type="text"><![CDATA[问题 要把三维空间中的点集\(\{p_1,p_2,\cdots,p_n\}\)通过旋转、平移和点集\(\{q_1,q_2,\cdots,q_n\}\)匹配起来。\(p\)可能是预先给定的模型，而\(q\)是在另一个坐标系中观察的结果，匹配就是要找到两个坐标系的转换关系。 给定对应关系的情况 先考虑简单的情形，即给定了两个集合中点的对应关系，\(p_i\)对应\(q_i\)。 定义两个点集间的距离为： \[ \sum_{i=1}^n\|p_i-q_i\|^2 \] 我们的目标是找到旋转矩阵\(R\)、平移向量\(b\)，使得\(p\)变换后与\(q\)的距离最小 \[ \min_{R,b}\sum_{i=1}^n\|Rp_i+b-q_i\|^2 \] 目标函数对\(b\)求导并令导数为\(\mathbf{0}\)，得： \[ b=\bar{q}-R\bar{p} \] 其中 \[ \bar{p}=\frac{1}{n}\sum_{i=1}^np_i\\ \bar{q}=\frac{1}{n}\sum_{i=1}^nq_i\\ \] 分别是点集的“质心”。 将\(b\)的表达式代入目标函数，得到： \[ \sum_{i=1}^n\|Rp_i+b-q_i\|=\sum_{i=1}^n\|Rp_i&#39;-q_i&#39;\|^2 \] 其中： \[ p_i&#39;=p_i-\bar{p}\\ q_i&#39;=q_i-\bar{q} \] 为点相对于质心的坐标。 \[ \begin{aligned} \sum_{i=1}^n\|Rp_i&#39;-q_i&#39;\|^2&amp;=\sum_{i=1}^n(Rp_i&#39;\cdot Rp_i&#39;)-2\sum_{i=1}^n(Rp_i&#39;\cdot q_i&#39;)+\sum_{i=1}^n(q_i&#39;\cdot q_i&#39;)\\ &amp;=\sum_{i=1}^n(\|p_i&#39;\|^2+\|q_i&#39;\|^2)-2\sum_{i=1}^nRp_i&#39;\cdot q_i&#39; \end{aligned} \] 上式中\(Rp_i&#39;\cdot Rp_i&#39;=\|p_i&#39;\|^2\)是因为旋转不改变向量长度。 上式第一项与\(R\)无关，因此只要最小化第二项。或者等价地求如下最大化： \[ \max_R\sum_{i=1}^nRp_i&#39;\cdot q_i&#39; \] 下面用两种方法来求解。 采用新符号\(x_i=p_i&#39;\)，\(y_i=q_i&#39;\) SVD 用SVD推导时似乎常引用资料[4]，不过我下面采用资料[3]中的推导，因为更简单直接。 \[ \begin{aligned} \sum_{i=1}^nRx_i\cdot y_i&amp;=\sum_{i=1}^ny_i^TRx_i\\ &amp;=tr(\sum_{i=1}^ny_i^TRx_i) \quad\text{标量=标量的迹}\\ &amp;=tr(\sum_{i=1}^nx_iy_i^TR) \quad\text{因为tr(AB)=tr(BA)}\\ &amp;=tr(CR) \quad C\triangleq\sum_{i=1}^nx_iy_i^T\\ &amp;=tr(U\Lambda V^TR) \quad\text{SVD分解：$C=U\Lambda V^T$}\\ &amp;=tr(\Lambda V^TRU) \\ &amp;=tr(\Lambda T) \quad T\triangleq V^TRU\\ &amp;=\sum_{i=1}^3\lambda_iT_{ii} \end{aligned} \] 由于\(V,R,U\)都是正交矩阵，因此\(T\)是正交矩阵，即每一列的长度为1，则\(T_{ii}\leq 1\)，因此： \[ \sum_{i=1}^3\lambda_iT_{ii}\leq\sum_{i=1}^3\lambda_i \] 当\(T_{ii}=1\)时等号成立，此时\(T\)的其他元素为0，即\(T=I\)。由\(V^TRU=T=I\)，得旋转矩阵： \[ R=VU^T \] 当然旋转矩阵应该满足\(\det(R)=1\)。如果求得\(\det (VU^T)=-1\)，那说明\(T\)不能等于\(I\)。 \[ \begin{aligned} \det(T)&amp;=\det(V^TRU) \\ &amp;=\det(UV^TR) \\ &amp;=\det(UV^T)\det(R) \\ &amp;=-1 \end{aligned} \] 在这种情况下[3]中说 It is easy to see what the second largest value is 就是\(T_{11}=T_{22}=1,T_{33}=-1\)，不过我没有看出来为什么是这样。。 那相应的\(R\)变成 \[ R=V\begin{bmatrix} 1 &amp; &amp; \\ &amp; 1 &amp; \\ &amp; &amp; -1 \end{bmatrix}U^T \] [4]中说如果是有点集共面因而\(\lambda_3=0\)（因为此时\(C\)不是满秩），那么对结果没有影响。如果\(C\)满秩而反射（即\(\det(R)=-1\)）是最优解，那么很可能数据中有outlier。 四元数 用\(q\)来表示四元数。 由四元数表示旋转的方式得到下面的目标函数： \[ \sum_{i=1}^n(qx_iq^*)\cdot y_i \] 可以证明： \[ (qx_iq^*)\cdot y_i=(qx_i)\cdot(y_iq) \] 因此： \[ \begin{aligned} \sum_{i=1}^n(qx_iq^*)\cdot y_i &amp;=\sum_{i=1}^n(qx_i)\cdot(y_iq)\\ &amp;=\sum_{i=1}^n([x_i]_Rq)\cdot([y_i]_Lq)\\ &amp;=\sum_{i=1}^nq^T[x_i]_R^T[y_i]_Lq\\ &amp;=q^T\left(\sum_{i=1}^n[x_i]_R^T[y_i]_L\right)q\\ &amp;\triangleq q^TMq \end{aligned} \] 可以验证\([x_i]_R^T[y_i]_L\)是对称阵，因此\(M\)是对称阵。 那剩下的就是线代的标准内容了，\(q^TMq\)能取得的最大值为\(M\)的最大的特征值，对应的\(q\)为对应的归一化的特征向量。 ICP 接下来就是不知道对应点的情况，也就是ICP要解决的。 这部分比较简单。既然不知道\(p_i\)对应的\(q\)，那我可以把\(p_i\)与点集\(q\)中离它最近的点对应。假设这是在第\(k\)轮，这样得到的距离记为\(c_k\)（c代表correspondence）。 有了对应关系，就可以按上面的方法做配准，记得到的距离为\(d_k\)。因为我们做了最小化，显然 \[ d_k\leq c_k \] 对于变换后的点集\(p\)中的每个点，我们再在\(q\)中找距离最近的点。这样显然每个\(p_i\)对应的距离都只可能减小，因此总的距离也只会减小，即： \[ c_{k+1}\leq d_k \] 这样， \[ 0\leq d_{k+1} \leq c_{k+1} \leq d_k\leq c_k \] 单调下降有下界，因此会收敛到一个局部极小值。 实现时设定一个阈值\(\tau\)，当满足如下条件时停止迭代。 \[ d_k-d_{k+1}&lt;\tau \] 附录 四元数性质证明 只要证明： \[ (pq)\cdot r=p\cdot(rq^*) \] 然后代入 \[ p\leftarrow qx_i\\ q\leftarrow q^*\\ r\leftarrow y_i \] 就得证。 由于 \[ (pq)\cdot r = ([q]_Rp)^Tr=p^T[q]_R^Tr\\ p\cdot(rq^*)=p^T[q^*]_Rr \] 因此只要证： \[ [q]_R^T=[q^*]_R \] 而这是显然的。 对称阵验证 因为\(x\),\(y\)都是虚四元数， \[ \begin{aligned} \left[x\right]_R^\top[y]_L&amp;=\begin{bmatrix} 0 &amp; \mathbf{x}_v^\top\\ -\mathbf{x}_v &amp; [\mathbf{x}_v]_{\times} \end{bmatrix}\begin{bmatrix} 0 &amp; -\mathbf{y}_v^\top\\ \mathbf{y}_v &amp; [\mathbf{y}_v]_{\times} \end{bmatrix}\\ &amp;=\begin{bmatrix} \mathbf{x}_v^\top\mathbf{y}_v &amp; \mathbf{x}_v^\top[\mathbf{y}_v]_{\times}\\ [\mathbf{x}_v]_{\times}\mathbf{y}_v &amp; \mathbf{x}_v\mathbf{y}_v^\top+[\mathbf{x}_v]_{\times}[\mathbf{y}_v]_{\times} \end{bmatrix} \end{aligned} \] 其它都比较明显，只要证明 \[ \mathbf{x}_v\mathbf{y}_v^\top+[\mathbf{x}_v]_{\times}[\mathbf{y}_v]_{\times} \] 是对称阵。由于 \[ [\mathbf{x}_v]_{\times}[\mathbf{y}_v]_{\times}=\mathbf{y}_v\mathbf{x}_v^\top-(\mathbf{x}_v\cdot\mathbf{y}_v)I \] 因此结论成立。 参考资料 四元数用于求解shape registration A Method for Registration of 3-D Shapes（ICP原始文献） SVD方法推导 Least-Squares Fitting of Two 3-D Point Sets （另一种SVD推导） Closed-form solution of absolute orientation using unit quaternions （四元数推导的原始文献）]]></content>
      <categories>
        <category>SLAM</category>
      </categories>
      <tags>
        <tag>ICP</tag>
        <tag>SVD</tag>
        <tag>四元数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[三维旋转]]></title>
    <url>%2F2019%2F08%2F10%2F%E4%B8%89%E7%BB%B4%E6%97%8B%E8%BD%AC%2F</url>
    <content type="text"><![CDATA[旋转三维向量 图中\(\mathbf{u}\)为单位向量，表示转轴。将\(\mathbf{x}\)绕\(\mathbf{u}\)逆时针旋转角度\(\phi\)得到\(\mathbf{x}&#39;\)。可将\(\mathbf{x}\)分解为沿转轴的分量\(\mathbf{x}_{\parallel}\)和垂直转轴的分量\(\mathbf{x}_{\perp}\)。\(\mathbf{x}_{\parallel}\)在转动时不变，\(\mathbf{x}_{\perp}\)在平面上旋转角度\(\phi\)得到\(\mathbf{x}_{\perp}&#39;\)，可得 \[ \mathbf{x}&#39;=\mathbf{x}_{\parallel}+\mathbf{x}_{\perp}\cos\phi+(\mathbf{u}\times\mathbf{x})\sin\phi \label{3d-rotation}\tag{1} \] 旋转群 旋转保持： 向量长度 两向量的内积 相对方向 \(\mathbf{u}\times \mathbf{v}=\mathbf{w}\Longleftrightarrow r(\mathbf{u})\times r(\mathbf{v})=r(\mathbf{w})\) 1,2是等价的。1-&gt;2可由\(\|r(\mathbf{u-v})\|=\|\mathbf{u-v}\|\)得到，2-&gt;1可由\(r(\mathbf{u})\cdot r(\mathbf{u})=\mathbf{u}\cdot \mathbf{u}\)得到。 因此可定义旋转群： \[ SO(3):\{\mathbb{R}^3\to\mathbb{R}^3\mid\forall \mathbf{v},\mathbf{w}\in\mathbb{R}^3, \|r(\mathbf{v})\|=\|\mathbf{v}\|,r(\mathbf{v})\times r(\mathbf{w})=r(\mathbf{v}\times\mathbf{w})\} \] 旋转矩阵 算符\(r\)是线性的，可用矩阵\(R\)表示， \[ r(\mathbf{v})=\mathbf{Rv} \] 由 \[ (\mathbf{Rv})^\top(\mathbf{Rv})=\mathbf{v}^\top\mathbf{R}^\top\mathbf{R}\mathbf{v}=\mathbf{v}^\top\mathbf{v} \] 可得： \[ \mathbf{R}^\top\mathbf{R}=I \] 即旋转矩阵是正交矩阵。 由旋转性质3，对于向量\(\mathbf{u},\mathbf{v},\mathbf{w}\)组成的六面体，旋转前后的有向体积应该相等，即 \[ \begin{vmatrix} Ru &amp; Rv &amp; Rw \end{vmatrix}=\det(R) \begin{vmatrix} u &amp; v &amp; w \end{vmatrix}= \begin{vmatrix} u &amp; v &amp; w \end{vmatrix} \] 得\(\det(R)=1\)。 这就构成了\(SO(3)\)群(Special Orthogonal group)，其中的special就是指\(\det(R)=1\)。 指数映射 \[ \frac{d}{dt}(R^TR)=\dot{R}^TR+R^T\dot{R}=0 \] 得： \[ R^T\dot{R}=-(R^T\dot{R})^T \] 即\(R^T\dot{R}\)是反对称矩阵。这些反对称矩阵集合用\(\mathfrak{so}(3)\)表示，称为\(SO(3)\)的李代数。 反对称矩阵可以写成\([\omega]_{\times}\)的形式，即 \[ R^T\dot{R}=[\omega]_{\times} \] 得到： \[ \dot{R}=R[\omega]_{\times} \] 当\(R=I\)时，\(\dot{R}=[\omega]_{\times}\)，即李代数是在幺元处的切空间。 如果\(\omega\)为常数，上述方程解得： \[ R(t)=R(0)e^{[\omega]_{\times}t} \] 这称为指数映射： \[ \exp: \mathfrak{so}(3)\to SO(3);[\phi]_{\times}\mapsto \exp([\phi]_{\times})=e^{[\phi]_{\times}} \] 还可以定义"大写的"指数映射： \[ \text{Exp}: \mathbb{R}^3\to SO(3);\phi\mapsto\text{Exp}(\phi)=e^{[\phi]_{\times}} \] 如果绕转轴\(\mathbf{u}\)转了角度\(\phi\)，那么旋转矩阵： \[ \mathbf{R}=e^{\phi[\mathbf{u}]_{\times}} \] 上式按泰勒展开后得： \[ \mathbf{R}=\mathbf{I}+\sin\phi[\mathbf{u}]_{\times}+(1-\cos\phi)[\mathbf{u}]_{\times}^2 \] 这就是Rodrigues旋转公式。推导过程用到了 \[ [\mathbf{a}]_{\times}^2=\mathbf{a}\mathbf{a}^\top-\mathbf{a}^\top\mathbf{aI} \] 根据这个式子，又可写成： \[ \mathbf{R}=\cos\phi\mathbf{I}+\sin\phi[\mathbf{u}]_{\times}+(1-\cos\phi)\mathbf{u}\mathbf{u}^\top \] 对数映射 从\(\mathbf{R}\)得到\(\phi\)和\(\mathbf{u}\) \[ \phi=\arccos\left(\frac{tr(\mathbf{R})-1}{2}\right)\\ \mathbf{u}=\frac{(\mathbf{R}-\mathbf{R}^T)^\vee}{2\sin\phi} \] 其中\(\bullet ^\vee\)是\([\bullet]_{\times}\)的逆，即\(([\mathbf{v}_{\times}]^\vee)=\mathbf{v}\). 旋转作用 将\(\mathbf{R}=\text{Exp}(\mathbf{u}\phi)\)作用在向量\(\mathbf{x}\)上，得到： \[ \begin{aligned} \mathbf{x}&#39;&amp;=\mathbf{R}\mathbf{x}\\ &amp;=(\mathbf{I}+\sin\phi[\mathbf{u}]_{\times}+(1-\cos\phi)[\mathbf{u}]_{\times}^2)\mathbf{x}\\ &amp;=\mathbf{x}_{\parallel}+\mathbf{x}_{\perp}\cos\phi+(\mathbf{u}\times\mathbf{x})\sin\phi \end{aligned} \] 与式\((\ref{3d-rotation})\)一致。 四元数 旋转公式为： \[ \mathbf{x}&#39;=\mathbf{q}\otimes\mathbf{x}\otimes\mathbf{q}^* \label{quat-rotation}\tag{2} \] 由于 \[ \|\mathbf{x}&#39;\|=\|\mathbf{q}\|^2\|\mathbf{x}\|=\|\mathbf{x}\| \] 因此\(\|\mathbf{q}\|^2=1\)，即\(\mathbf{q}\)是单位四元数： \[ \mathbf{q}^*\otimes\mathbf{q}=1=\mathbf{q}\otimes\mathbf{q}^* \] 这与\(R^TR=I=RR^T\)的条件类似。 还可以看到，自动保持了相对方向： \[ \begin{aligned} r(v)\times r(w)&amp;=(q\otimes v\otimes q^*)\times(q\otimes w\otimes q^*)\\ &amp;=\frac{1}{2}\left((q\otimes v\otimes q^*)\otimes(q\otimes w\otimes q^*)-(q\otimes w\otimes q^*)\otimes(q\otimes v\otimes q^*)\right)\\ &amp;=\frac{1}{2}(q\otimes v\otimes w\otimes q^*-q\otimes w\otimes v\otimes q^*)\\ &amp;=\frac{1}{2}(q\otimes(v\otimes w-w\otimes v)\otimes q^*)\\ &amp;=q\otimes(v\times w)\otimes q^*\\ &amp;=r(v\times w) \end{aligned} \] 指数映射 \[ \frac{d(q^*\otimes q)}{dt}=\dot{q}^*\otimes q+q^*\otimes\dot{q}=0 \] 得： \[ q^*\otimes\dot{q}=-(\dot{q}^*\otimes q)=-(q^*\otimes\dot{q})^* \] 即\(q^*\otimes\dot{q}\)是虚四元数。令： \[ q^*\otimes\dot{q}=\Omega \] 得到： \[ \dot{q}=q\otimes\Omega \] 当\(q=1\)时，\(\dot{q}=\Omega\)，可见虚四元数构成了单位四元数球\(S^3\)的切空间。 如果\(\Omega\)为常数，上式解得\(q(t)=q(0)\otimes e^{\Omega t}\)，这就引出了指数映射。 如果绕转轴\(\mathbf{u}\)转了角度\(\phi\)，定义“大写的”指数映射： \[ \mathbf{q}\triangleq \text{Exp}(\phi\mathbf{u})=e^{\phi\mathbf{u}/2}=\cos\frac{\phi}{2}+\mathbf{u}\sin\frac{\phi}{2} \label{quat-form}\tag{3} \] 旋转作用 将式\((\ref{quat-form})\)代入式\((\ref{quat-rotation})\)， 推导可得式\((\ref{3d-rotation})\)，这就验证了正确性。 在证明中有一步\(\mathbf{u}\otimes\mathbf{x}\otimes\mathbf{u}=\mathbf{x}(\mathbf{u}^T\mathbf{u})-2\mathbf{u}(\mathbf{u}^T\mathbf{x})\)用到了\((a\times b)\times c=-a(c\cdot b)+b(c\cdot a)\) 四元数到旋转矩阵的转换 由 \[ \mathbf{q}\otimes\mathbf{x}\otimes\mathbf{q}^*=[\mathbf{q}^*]_R[\mathbf{q}]_L \begin{bmatrix} 0\\ \mathbf{x} \end{bmatrix}= \begin{bmatrix} 0\\ \mathbf{\mathbf{R}x} \end{bmatrix} \] 可以得到： \[ \mathbf{R}=(q_w^2-\mathbf{q}_v^\top\mathbf{q}_v)\mathbf{I}+2\mathbf{q}_v\mathbf{q}_v^\top+2q_w[\mathbf{q}_v]_\times \] 旋转合成 四元数和旋转矩阵的合成顺序一样： \[ \mathbf{R}\{\mathbf{q}_2\otimes\mathbf{q}_1\}=\mathbf{R}\{\mathbf{q}_2\}\mathbf{R}\{\mathbf{q}_1\} \] 这是因为\(\mathbf{q}_2\otimes\mathbf{q}_1\)作用于\(\mathbf{x}\)时，是\(\mathbf{q}_1\)先作用： \[ \mathbf{q}_2\otimes\mathbf{q}_1\otimes\mathbf{x}\otimes(\mathbf{q}_2\otimes\mathbf{q}_1)^*=\mathbf{q}_2\otimes(\mathbf{q}_1\otimes\mathbf{x}\otimes\mathbf{q}_1^*)\otimes\mathbf{q}_2^* \] 参考资料 Quaternion kinematics for the error-state Kalman filter]]></content>
      <tags>
        <tag>四元数</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[EM算法]]></title>
    <url>%2F2019%2F07%2F24%2FEM%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[EM算法就是含有隐变量的概率模型参数的极大似然估计法（或极大后验概率估计法） 用\(x\)表示观测变量（的数据），\(z\)表示隐变量，\(\theta\)表示需要估计的模型参数。 给定观测数据\(x\)，其似然函数为\(p(x|\theta)\)，对数似然函数\(L(\theta) \triangleq \ln p(x|\theta)\)。 极大似然估计： \[ \max_{\theta}L(\theta) \] EM算法 算法是迭代进行的，假设第\(n\)轮迭代得到的参数估计为\(\theta_n\)，希望下一轮迭代使得\(L(\theta)&gt;L(\theta_n)\) \[ \begin{aligned} L(\theta)-L(\theta_n) &amp;=\ln \left(\sum_zp(z,x|\theta)\right)-\ln p(x|\theta_n)\\ \end{aligned} \] 对数函数是凹函数，满足Jensen不等式： \[ \log \left(\sum_j\lambda_jy_j\right)\geq \sum_j \lambda_j\log y_j \] 其中\(\lambda_j\geq 0, \sum_j\lambda_j=1\) 那么 \[ \begin{aligned} L(\theta)-L(\theta_n) &amp;=\ln \left(\sum_zp(z|x,\theta_n)\frac{p(z,x|\theta)}{p(z|x,\theta_n)}\right)-\ln p(x|\theta_n)\\ &amp;\geq \sum_zp(z|x,\theta_n)\ln\frac{p(z,x|\theta)}{p(z|x,\theta_n)}-\ln p(x|\theta_n)\\ &amp;=\sum_zp(z|x,\theta_n)\ln\frac{p(z,x|\theta)}{p(z|x,\theta_n)}-\left(\sum_zp(z|x,\theta_n)\right)\ln p(x|\theta_n)\\ &amp;=\sum_zp(z|x,\theta_n)\ln\frac{p(z,x|\theta)}{p(z,x|\theta_n)} \end{aligned} \] 即： \[ \begin{aligned} L(\theta)&amp;\geq L(\theta_n)+\sum_zp(z|x,\theta_n)\ln\frac{p(z,x|\theta)}{p(z,x|\theta_n)}\\ &amp;\triangleq B(\theta,\theta_n) \end{aligned} \] 也就是\(B(\theta,\theta_n)\)是\(L(\theta)\)的一个下界函数。而且容易看到： \[ B(\theta_n,\theta_n) = L(\theta_n) \] 即自变量等于\(\theta_n\)时，两个函数值是相等的。 那么如果找一个\(\theta_{n+1}\)，使得\(B(\theta_{n+1},\theta_n)\geq B(\theta_n,\theta_n)\)，就能使\(L(\theta)\)增大： \[ \begin{aligned} L(\theta_{n+1})&amp;\geq B(\theta_{n+1},\theta_n)\\ &amp;\geq B(\theta_n,\theta_n)\\ &amp;=L(\theta_n) \end{aligned} \] 一种方法是使： \[ \theta_{n+1}=\operatorname{argmax} B(\theta,\theta_n) \] 但是注意这样并不能保证\(L(\theta)\)的增长也能极大化，这和\(L(\theta)\)的具体形状有关。 使\(B(\theta,\theta_n)\)极大化，等价于极大化下面的式子，因为其他项为常数 \[ Q(\theta,\theta_n)\triangleq \sum_zp(z|x,\theta_n)\ln p(z,x|\theta) \] 这个式子有着鲜明的含义： 当有了\(x,\theta\)，根据\(z\)的概率分布\(p(z|x,\theta_n)\)可以采样出\(z\)，这样就有了完整的数据，可以做平常的极大似然估计。 因此EM算法的步骤是： E步：求完整数据对数似然的期望，这个期望是在概率分布\(p(z|x,\theta_n)\)下求的。 M步：极大化，只是保证\(L(\theta)\)增大的一种方式。 在E步中，\(\ln p(x,z|\theta)\)应该是很好算的，\(p(z|x,\theta)\)也好算： \[ p(z|x,\theta)=\frac{p(z,x|\theta)}{p(x|\theta)}=\frac{p(z,x|\theta)}{\sum_{z&#39;}p(z&#39;,x|\theta)} \]]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Yet Again a Subarray Problem]]></title>
    <url>%2F2019%2F07%2F24%2FYet%20Again%20a%20Subarray%20Problem%2F</url>
    <content type="text"><![CDATA[题目 题意 给定一个数组\(A\)，长度\(N&lt;2000\)，\(A_i&lt;2000\)。对于它的每个连续子序列，计算这个子序列中第\(K\)小的数（\(K\)随子序列而不同，这不是问题重点），记为\(x\)，得到\(x\)在子序列中出现的次数\(F\)。如果\(F\)在子序列中也出现过，那么答案加一，求总的答案。 题解 解法一 线段树 对于左端点下标为\(i\)的子序列，在递增右端点的过程中，可以连续处理。线段树中节点保存的是值在\([l,r]\)之间的数在当前子序列中出现的次数，而不是序列中的下标。那么根据左子节点中保存的数和\(K\)的关系，很容易找到第\(K\)小的数对应的叶节点，也就得到\(F\)。然后再看\(F\)对应的叶节点中保存的出现次数是否为0即可。 解法二 预处理+二分 首先预处理得到PRE[x][r]，表示\([1,r]\)子序列中\(\leq x\)的数的个数。预处理后可以\(O(1)\)计算\([l,r]\)子序列中\(\leq x\)的数的个数。二分\(x\)就可得到第\(K\)小的数。根据PRE[x][r]也可以\(O(1)\)得到\([l,r]\)子序列中等于\(x\)的数的个数，因此问题解决。]]></content>
      <categories>
        <category>算法题</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[最长上升子序列(LIS)的nlogn算法]]></title>
    <url>%2F2019%2F07%2F24%2F%E6%9C%80%E9%95%BF%E4%B8%8A%E5%8D%87%E5%AD%90%E5%BA%8F%E5%88%97(LIS)%E7%9A%84nlogn%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[序列用\(A\)表示，处理到当前元素为止，长度为\(i\)的子序列的最小的结尾元素用\(B_i\)表示。序列\(B\)是单调增的，即 \[ i&lt;j\Leftrightarrow B_i&lt;B_j \] 因为如果\(B_i\geq B_j\)，那么\(B_j\)对应的子序列中的第\(i\)个数是小于\(B_i\)的，这与\(B_i\)是最小的矛盾。 假设当前LIS长度为\(L\)，现在要计算以\(A_k\)结尾的LIS，只需二分长度\(l\)，看是否满足\(B_l&lt;A_k\)。假设以\(A_k\)结尾的LIS长度为\(m\)，再进行一次更新： 12B[m] = min(B[m], A[k]);L = max(L, m); 即可。]]></content>
      <categories>
        <category>算法</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Biject Inside]]></title>
    <url>%2F2019%2F07%2F24%2FBiject%20Inside%2F</url>
    <content type="text"><![CDATA[题目地址 又是想了很久也没做出来，写下官方题解吧。 题意 A随机选择1 ~ n的一个排列\(p\)，B随机选择1 ~ n的一个子集\(S\)，如果存在\(1\leq i \leq n\)，使得\(i,p_i\)都在\(S\)中，A就赢了。问A赢的概率。 题解 给定一个排列\(p\)，要求有哪些集合使得A能赢是困难的，因为这和具体的排列有关。可以反过来，即给定\(B\)的集合\(S\)，求有多少种排列能让A赢，这个数只和\(|S|\)有关，因此容易处理。 下面先计算使A输的情况，因为这容易算。 假设\(S\)的大小为\(k\)，令\(i=S_j\)，其中\(1\leq j \leq k\)。那么\(p_i\)不能取\(S\)中的元素，只能从剩下的\(n-k\)个元素中选。这样的数量为\(A_{n-k}^k\)。剩下的\(n-k\)个\(i\)对应的\(p_i\)可以随便选，为\((n-k)!\)，因此一个大小为\(k\)的集合让A输的排列有\(A_{n-k}^k(n-k)!\)种。再考虑到大小为\(k\)的集合有\(C_{n}^k\)个，最后对\(k\)求和，总数为： \[ \displaystyle\sum_{k=0}^n C_{n}^kA_{n-k}^k(n-k)!=n!\sum_{k=0}^nC_{n-k}^k \] 需要计算\(\sum_{k=0}^nC_{n-k}^k\triangleq g_n\)，这实际上和Fibonacci数\(F_n\)有关。下面证明（来源）。 考虑由0,1构成、没有1相邻的n位字符串的数量，记为\(f_{n}\)。 第一部分 建立\(g_n\)和\(f_n\)的联系。 计算有\(k\)个1的串的数量：那么有\(n-k\)个0，算上两端有\(n-k+1\)个隔板，把\(k\)个1放入隔板中，有\(C_{n-k+1}^k\)种方法。枚举\(k\)，总数为\(\sum_{k=0}^nC_{n-k+1}^k=g_{n+1}\)。 因此\(g_{n}=f_{n-1}\) 第二部分 证明\(f_{n}\)是Fibonacci数。 用\(f_{n,0}\)表示串以0结尾的答案，\(f_{n,1}\)表示以1结尾的答案。 显然 \[ f_{n,0}=f_{n-1}\\ f_{n,1}=f_{n-1,0} \] 相加得： \(f_n=f_{n-1}+f_{n-1,0}=f_{n-1}+f_{n-2}\) 而\(f_1=2,f_2=3\) 如果按Fibonacci数的定义，就有\(f_n=F_{n+2}\)。 由证明第一部分，得\(g_n=F_{n+1}\)]]></content>
      <categories>
        <category>算法题</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Superior Substring]]></title>
    <url>%2F2019%2F07%2F24%2FSuperior%20Substring%2F</url>
    <content type="text"><![CDATA[题目地址 题意 核心问题是，给定一个长为\(N\)的01串，连续子串中1的数量大于等于子串长度一半（向下取整）的为合法串。问合法串的最大长度。 解法 \(N\leq 10^5\)，因此需要\(n\log n\)的算法。 我原以为这个长度会满足单调性，即如果长的串满足，短的一定满足。其实不对。比如10001，整个串满足条件，但找不到长度为4的合法串。 下面给出官方题解。 用\(f\)表示累积和序列，那么对于\([L,R]\)这个子串，原条件可表示为： \[ f(R)-f(L-1)\geq \left\lfloor\frac{R-L+1}{2}\right\rfloor \quad(1) \] 这个条件可以等价表示为 \[ f(R)-f(L-1)\geq \frac{R-L}{2} \quad(2) \] 证明 (1)-&gt;(2) 因为\(\left\lfloor\frac{R-L+1}{2}\right\rfloor\geq \frac{R-L}{2}\)（可根据\(R-L\)的奇偶讨论）。 (2)-&gt;(1) 因为\(f(R)-f(L-1)\)为整数，(2)实际上是\(f(R)-f(L-1)\geq \left\lceil\frac{R-L}{2}\right\rceil=\left\lfloor\frac{R-L+1}{2}\right\rfloor\)。 证毕。 (2)可变为 \[ 2f(R)-R\geq 2f(L-1)-L \] 那么可形成两个这样的序列： \[ p(i)=2f(i)-i\\ q(i)=2f(i-1)-i \] 为了得到以\(R\)结尾的最长子串，就是要计算最小的满足\(p(R)\geq q(L)\)的\(L\)。 当然这里要用二分来计算\(L\)，否则就不能降低复杂度。然而\(q\)并不是一个单调序列。 实际上不需要完整的\(q\)序列。如果\(q\)中存在递增部分：\(i&lt;j\Rightarrow q(i)&lt;q(j)\)，那么\(p(R)\geq q(j)\Rightarrow p(R)\geq q(i)\)且\([i,R]\)比\([j,R]\)更长，因此不会选到\(j\)。因此只要从\(i=1\)开始，形成一个单调递减子序列\(q&#39;\)，就可以做二分了。 官方的程序用了一个比较聪明的做法，不是先提取单调子序列，而是改变元素值，使原序列成为（非严格）单调序列。方法是：如果下一个元素比现在的大，那么下一个元素变为与现在的一样大。]]></content>
      <categories>
        <category>算法题</category>
      </categories>
  </entry>
</search>
